{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: 다층 퍼셉트론 (MLP) - 딥러닝의 시작\n",
    "\n",
    "이제 진짜 딥러닝의 세계로 들어갑니다! 다층 퍼셉트론(Multi-Layer Perceptron)과 역전파(Backpropagation) 알고리즘을 직접 구현해봅시다.\n",
    "\n",
    "## 학습 목표\n",
    "1. 순전파(Forward Propagation) 이해하고 구현하기\n",
    "2. 역전파(Backpropagation) 알고리즘 이해하기\n",
    "3. 경사하강법(Gradient Descent)으로 학습하기\n",
    "4. 실제 분류 문제 해결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 시각화 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 다층 퍼셉트론의 구조\n",
    "\n",
    "다층 퍼셉트론은 입력층, 은닉층(들), 출력층으로 구성됩니다.\n",
    "\n",
    "### 순전파 과정:\n",
    "1. 입력층 → 은닉층: $h = f(W_1 \\cdot x + b_1)$\n",
    "2. 은닉층 → 출력층: $y = f(W_2 \\cdot h + b_2)$\n",
    "\n",
    "여기서 $f$는 활성화 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수와 그 미분\n",
    "def sigmoid(x):\n",
    "    \"\"\"시그모이드 함수\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # 오버플로우 방지\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"시그모이드 함수의 미분\"\"\"\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU 함수\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"ReLU 함수의 미분\"\"\"\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Tanh 함수\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"Tanh 함수의 미분\"\"\"\n",
    "    return 1 - x**2\n",
    "\n",
    "# 활성화 함수 시각화\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# 함수값\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, sigmoid(x), label='Sigmoid', linewidth=2)\n",
    "plt.plot(x, relu(x), label='ReLU', linewidth=2)\n",
    "plt.plot(x, tanh(x), label='Tanh', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Activation Functions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 미분값\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, sigmoid_derivative(sigmoid(x)), label=\"Sigmoid'\", linewidth=2)\n",
    "plt.plot(x, relu_derivative(x), label=\"ReLU'\", linewidth=2)\n",
    "plt.plot(x, tanh_derivative(tanh(x)), label=\"Tanh'\", linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.title('Derivatives of Activation Functions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP 클래스 구현\n",
    "\n",
    "이제 다층 퍼셉트론을 처음부터 구현해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1, activation='sigmoid'):\n",
    "        \"\"\"\n",
    "        다층 퍼셉트론 초기화\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size: 입력층 크기\n",
    "        - hidden_size: 은닉층 크기\n",
    "        - output_size: 출력층 크기\n",
    "        - learning_rate: 학습률\n",
    "        - activation: 활성화 함수 ('sigmoid', 'relu', 'tanh')\n",
    "        \"\"\"\n",
    "        # 가중치 초기화 (Xavier 초기화)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # 활성화 함수 설정\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_derivative = sigmoid_derivative\n",
    "        elif activation == 'relu':\n",
    "            self.activation = relu\n",
    "            self.activation_derivative = relu_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_derivative = tanh_derivative\n",
    "        \n",
    "        # 학습 과정 기록\n",
    "        self.losses = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        순전파\n",
    "        \"\"\"\n",
    "        # 입력층 → 은닉층\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.activation(self.z1)\n",
    "        \n",
    "        # 은닉층 → 출력층\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)  # 출력층은 항상 시그모이드 (이진 분류)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        \"\"\"\n",
    "        역전파\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # 샘플 수\n",
    "        \n",
    "        # 출력층 오차\n",
    "        self.dz2 = output - y\n",
    "        self.dW2 = (1/m) * np.dot(self.a1.T, self.dz2)\n",
    "        self.db2 = (1/m) * np.sum(self.dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # 은닉층 오차\n",
    "        da1 = np.dot(self.dz2, self.W2.T)\n",
    "        self.dz1 = da1 * self.activation_derivative(self.a1)\n",
    "        self.dW1 = (1/m) * np.dot(X.T, self.dz1)\n",
    "        self.db1 = (1/m) * np.sum(self.dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        # 가중치 업데이트\n",
    "        self.W2 -= self.learning_rate * self.dW2\n",
    "        self.b2 -= self.learning_rate * self.db2\n",
    "        self.W1 -= self.learning_rate * self.dW1\n",
    "        self.b1 -= self.learning_rate * self.db1\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, verbose=True):\n",
    "        \"\"\"\n",
    "        모델 학습\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # 순전파\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # 손실 계산 (Binary Cross-Entropy)\n",
    "            loss = -np.mean(y * np.log(output + 1e-8) + (1 - y) * np.log(1 - output + 1e-8))\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            # 역전파\n",
    "            self.backward(X, y, output)\n",
    "            \n",
    "            # 진행상황 출력\n",
    "            if verbose and epoch % 100 == 0:\n",
    "                accuracy = self.accuracy(X, y)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        예측\n",
    "        \"\"\"\n",
    "        output = self.forward(X)\n",
    "        return (output > 0.5).astype(int)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        정확도 계산\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XOR 문제 해결하기\n",
    "\n",
    "이제 퍼셉트론으로는 해결할 수 없었던 XOR 문제를 해결해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR 데이터\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# MLP 모델 생성 및 학습\n",
    "mlp_xor = MLP(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)\n",
    "mlp_xor.train(X_xor, y_xor, epochs=5000, verbose=False)\n",
    "\n",
    "# 결과 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# 학습 곡선\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(mlp_xor.losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 결정 경계\n",
    "plt.subplot(1, 3, 2)\n",
    "x_min, x_max = -0.5, 1.5\n",
    "y_min, y_max = -0.5, 1.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "Z = mlp_xor.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, levels=20, alpha=0.8, cmap='RdBu')\n",
    "plt.colorbar(label='Output')\n",
    "plt.scatter(X_xor[y_xor.ravel() == 0][:, 0], X_xor[y_xor.ravel() == 0][:, 1], \n",
    "            c='red', s=200, edgecolors='black', linewidths=2, label='0')\n",
    "plt.scatter(X_xor[y_xor.ravel() == 1][:, 0], X_xor[y_xor.ravel() == 1][:, 1], \n",
    "            c='blue', s=200, edgecolors='black', linewidths=2, label='1')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('XOR Decision Boundary')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 예측 결과\n",
    "plt.subplot(1, 3, 3)\n",
    "predictions = mlp_xor.predict(X_xor)\n",
    "output_probs = mlp_xor.forward(X_xor)\n",
    "table_data = [[x1, x2, y[0], pred[0], f\"{prob[0]:.3f}\"] \n",
    "              for (x1, x2), y, pred, prob in zip(X_xor, y_xor, predictions, output_probs)]\n",
    "plt.table(cellText=table_data,\n",
    "          colLabels=['x1', 'x2', 'Target', 'Prediction', 'Probability'],\n",
    "          cellLoc='center',\n",
    "          loc='center')\n",
    "plt.axis('off')\n",
    "plt.title('XOR Predictions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"최종 정확도: {mlp_xor.accuracy(X_xor, y_xor):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 더 복잡한 데이터셋으로 실험\n",
    "\n",
    "이제 더 복잡한 비선형 패턴을 가진 데이터를 분류해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 데이터셋 생성\n",
    "datasets = {\n",
    "    'moons': make_moons(n_samples=200, noise=0.2, random_state=42),\n",
    "    'circles': make_circles(n_samples=200, noise=0.2, factor=0.5, random_state=42)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx, (name, (X, y)) in enumerate(datasets.items()):\n",
    "    # 데이터 정규화\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    # 학습/테스트 분할\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 모델 학습\n",
    "    mlp = MLP(input_size=2, hidden_size=10, output_size=1, learning_rate=0.5, activation='tanh')\n",
    "    mlp.train(X_train, y_train, epochs=1000, verbose=False)\n",
    "    \n",
    "    # 원본 데이터 시각화\n",
    "    ax = axes[idx, 0]\n",
    "    ax.scatter(X[y.ravel() == 0][:, 0], X[y.ravel() == 0][:, 1], c='red', alpha=0.6, label='Class 0')\n",
    "    ax.scatter(X[y.ravel() == 1][:, 0], X[y.ravel() == 1][:, 1], c='blue', alpha=0.6, label='Class 1')\n",
    "    ax.set_title(f'{name.capitalize()} Dataset')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 결정 경계\n",
    "    ax = axes[idx, 1]\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = mlp.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, levels=20, alpha=0.8, cmap='RdBu')\n",
    "    ax.scatter(X[y.ravel() == 0][:, 0], X[y.ravel() == 0][:, 1], c='red', edgecolors='black', linewidths=1)\n",
    "    ax.scatter(X[y.ravel() == 1][:, 0], X[y.ravel() == 1][:, 1], c='blue', edgecolors='black', linewidths=1)\n",
    "    ax.set_title('Decision Boundary')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 학습 곡선\n",
    "    ax = axes[idx, 2]\n",
    "    ax.plot(mlp.losses)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Loss')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 정확도 출력\n",
    "    train_acc = mlp.accuracy(X_train, y_train)\n",
    "    test_acc = mlp.accuracy(X_test, y_test)\n",
    "    print(f\"{name.capitalize()} - Train Accuracy: {train_acc:.2%}, Test Accuracy: {test_acc:.2%}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 은닉층 뉴런 수의 영향\n",
    "\n",
    "은닉층의 뉴런 수가 모델의 표현력에 어떤 영향을 미치는지 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moons 데이터셋 사용\n",
    "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# 다양한 은닉층 크기 실험\n",
    "hidden_sizes = [1, 2, 5, 10, 20, 50]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, hidden_size in enumerate(hidden_sizes):\n",
    "    # 모델 학습\n",
    "    mlp = MLP(input_size=2, hidden_size=hidden_size, output_size=1, learning_rate=0.5)\n",
    "    mlp.train(X, y, epochs=1000, verbose=False)\n",
    "    \n",
    "    # 결정 경계 시각화\n",
    "    ax = axes[idx]\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = mlp.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, levels=20, alpha=0.8, cmap='RdBu')\n",
    "    ax.scatter(X[y.ravel() == 0][:, 0], X[y.ravel() == 0][:, 1], c='red', edgecolors='black', linewidths=1)\n",
    "    ax.scatter(X[y.ravel() == 1][:, 0], X[y.ravel() == 1][:, 1], c='blue', edgecolors='black', linewidths=1)\n",
    "    \n",
    "    accuracy = mlp.accuracy(X, y)\n",
    "    ax.set_title(f'Hidden Size: {hidden_size}\\nAccuracy: {accuracy:.2%}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 가중치 시각화\n",
    "\n",
    "학습된 가중치가 어떻게 생겼는지 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 2-4-1 네트워크로 XOR 학습\n",
    "mlp_vis = MLP(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)\n",
    "mlp_vis.train(X_xor, y_xor, epochs=5000, verbose=False)\n",
    "\n",
    "# 가중치 시각화\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# W1 가중치 행렬\n",
    "ax = axes[0]\n",
    "im1 = ax.imshow(mlp_vis.W1, cmap='RdBu', aspect='auto')\n",
    "ax.set_title('W1 (Input → Hidden)')\n",
    "ax.set_xlabel('Hidden Neurons')\n",
    "ax.set_ylabel('Input Features')\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_yticks(range(2))\n",
    "ax.set_yticklabels(['x1', 'x2'])\n",
    "plt.colorbar(im1, ax=ax)\n",
    "\n",
    "# 각 은닉 뉴런의 가중치 값\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        ax.text(j, i, f'{mlp_vis.W1[i, j]:.2f}', ha='center', va='center')\n",
    "\n",
    "# W2 가중치 행렬\n",
    "ax = axes[1]\n",
    "im2 = ax.imshow(mlp_vis.W2.T, cmap='RdBu', aspect='auto')\n",
    "ax.set_title('W2 (Hidden → Output)')\n",
    "ax.set_xlabel('Hidden Neurons')\n",
    "ax.set_ylabel('Output')\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_yticks([0])\n",
    "ax.set_yticklabels(['y'])\n",
    "plt.colorbar(im2, ax=ax)\n",
    "\n",
    "# 각 가중치 값\n",
    "for j in range(4):\n",
    "    ax.text(j, 0, f'{mlp_vis.W2[j, 0]:.2f}', ha='center', va='center')\n",
    "\n",
    "# 네트워크 다이어그램\n",
    "ax = axes[2]\n",
    "ax.set_title('Network Architecture')\n",
    "\n",
    "# 노드 위치\n",
    "layers = [2, 4, 1]\n",
    "layer_positions = [0, 1, 2]\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "# 노드와 연결선 그리기\n",
    "for l_idx, (layer_size, x_pos, color) in enumerate(zip(layers, layer_positions, colors)):\n",
    "    y_positions = np.linspace(0, 1, layer_size)\n",
    "    \n",
    "    # 노드 그리기\n",
    "    for y_pos in y_positions:\n",
    "        ax.scatter(x_pos, y_pos, s=500, c=color, edgecolors='black', linewidths=2, zorder=3)\n",
    "    \n",
    "    # 연결선 그리기\n",
    "    if l_idx < len(layers) - 1:\n",
    "        next_layer_size = layers[l_idx + 1]\n",
    "        next_y_positions = np.linspace(0, 1, next_layer_size)\n",
    "        \n",
    "        for y1 in y_positions:\n",
    "            for y2 in next_y_positions:\n",
    "                ax.plot([x_pos, layer_positions[l_idx + 1]], [y1, y2], \n",
    "                       'k-', alpha=0.3, linewidth=1)\n",
    "\n",
    "ax.set_xlim(-0.5, 2.5)\n",
    "ax.set_ylim(-0.2, 1.2)\n",
    "ax.axis('off')\n",
    "\n",
    "# 레이어 라벨\n",
    "ax.text(0, -0.1, 'Input\\nLayer', ha='center', fontsize=10)\n",
    "ax.text(1, -0.1, 'Hidden\\nLayer', ha='center', fontsize=10)\n",
    "ax.text(2, -0.1, 'Output\\nLayer', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 미니배치 학습 구현\n",
    "\n",
    "실제 딥러닝에서는 전체 데이터를 한 번에 학습하지 않고, 작은 배치로 나누어 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithMiniBatch(MLP):\n",
    "    def train_with_batches(self, X, y, epochs=1000, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        미니배치를 사용한 학습\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = n_samples // batch_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 데이터 셔플\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # 미니배치 학습\n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                \n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # 순전파\n",
    "                output = self.forward(X_batch)\n",
    "                \n",
    "                # 손실 계산\n",
    "                batch_loss = -np.mean(y_batch * np.log(output + 1e-8) + \n",
    "                                     (1 - y_batch) * np.log(1 - output + 1e-8))\n",
    "                epoch_loss += batch_loss\n",
    "                \n",
    "                # 역전파\n",
    "                self.backward(X_batch, y_batch, output)\n",
    "            \n",
    "            # 평균 손실\n",
    "            epoch_loss /= n_batches\n",
    "            self.losses.append(epoch_loss)\n",
    "            \n",
    "            # 진행상황 출력\n",
    "            if verbose and epoch % 100 == 0:\n",
    "                accuracy = self.accuracy(X, y)\n",
    "                print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# 더 큰 데이터셋으로 테스트\n",
    "X_large, y_large = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X_large = (X_large - X_large.mean(axis=0)) / X_large.std(axis=0)\n",
    "y_large = y_large.reshape(-1, 1)\n",
    "\n",
    "# 전체 배치 vs 미니배치 비교\n",
    "mlp_full = MLPWithMiniBatch(input_size=2, hidden_size=10, output_size=1, learning_rate=0.5)\n",
    "mlp_mini = MLPWithMiniBatch(input_size=2, hidden_size=10, output_size=1, learning_rate=0.5)\n",
    "\n",
    "print(\"전체 배치 학습:\")\n",
    "mlp_full.train(X_large, y_large, epochs=500, verbose=False)\n",
    "\n",
    "print(\"\\n미니배치 학습:\")\n",
    "mlp_mini.train_with_batches(X_large, y_large, epochs=500, batch_size=32, verbose=False)\n",
    "\n",
    "# 학습 곡선 비교\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mlp_full.losses, label='Full Batch', linewidth=2)\n",
    "plt.plot(mlp_mini.losses, label='Mini-Batch (size=32)', linewidth=2, alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Full Batch vs Mini-Batch Training')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n최종 정확도 - 전체 배치: {mlp_full.accuracy(X_large, y_large):.2%}\")\n",
    "print(f\"최종 정확도 - 미니배치: {mlp_mini.accuracy(X_large, y_large):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 연습 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 1: 드롭아웃(Dropout) 구현\n",
    "# 과적합을 방지하는 정규화 기법\n",
    "class MLPWithDropout(MLP):\n",
    "    def __init__(self, *args, dropout_rate=0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def forward_with_dropout(self, X, training=True):\n",
    "        # 힌트: training=True일 때만 드롭아웃 적용\n",
    "        # np.random.binomial(1, 1-self.dropout_rate, size=...) 사용\n",
    "        pass\n",
    "\n",
    "# 문제 2: 다중 클래스 분류를 위한 소프트맥스 출력층 구현\n",
    "def softmax(x):\n",
    "    # 힌트: exp(x) / sum(exp(x), axis=1)\n",
    "    # 수치 안정성을 위해 x - max(x) 먼저 계산\n",
    "    pass\n",
    "\n",
    "# 문제 3: 학습률 감소(Learning Rate Decay) 구현\n",
    "# 에폭이 진행될수록 학습률을 감소시키는 기법\n",
    "def learning_rate_decay(initial_lr, epoch, decay_rate=0.99):\n",
    "    # 힌트: initial_lr * (decay_rate ** epoch)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "이번 튜토리얼에서 배운 내용:\n",
    "1. 다층 퍼셉트론(MLP)의 구조와 순전파\n",
    "2. 역전파 알고리즘의 원리와 구현\n",
    "3. 경사하강법을 통한 가중치 학습\n",
    "4. 비선형 문제 해결 (XOR, Moons, Circles)\n",
    "5. 은닉층 크기의 영향\n",
    "6. 미니배치 학습\n",
    "\n",
    "### 핵심 개념:\n",
    "- **순전파**: 입력에서 출력까지 신호가 전달되는 과정\n",
    "- **역전파**: 오차를 이용해 가중치를 업데이트하는 과정\n",
    "- **활성화 함수**: 비선형성을 추가하여 복잡한 패턴 학습 가능\n",
    "- **경사하강법**: 손실함수를 최소화하는 최적화 알고리즘\n",
    "\n",
    "다음 단계에서는 PyTorch를 사용하여 더 효율적으로 신경망을 구현하는 방법을 배워보겠습니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}