{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: 대규모 언어 모델(LLM) 이해와 활용\n",
    "\n",
    "이제 실제 대규모 언어 모델들을 이해하고 활용하는 방법을 배워봅시다.\n",
    "\n",
    "## 학습 목표\n",
    "1. 현대 LLM의 발전 과정과 종류\n",
    "2. Hugging Face Transformers 사용법\n",
    "3. Fine-tuning과 Prompt Engineering\n",
    "4. LLM의 한계와 윤리적 고려사항\n",
    "5. 실제 응용 사례 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치 (처음 실행 시)\n",
    "# !pip install transformers datasets accelerate sentencepiece\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "\n",
    "# 시각화 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LLM의 발전 역사와 주요 모델들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_llm_evolution():\n",
    "    \"\"\"LLM 발전 과정 시각화\"\"\"\n",
    "    \n",
    "    # 주요 모델들의 정보\n",
    "    models = [\n",
    "        {'name': 'GPT', 'year': 2018, 'params': 0.117, 'company': 'OpenAI'},\n",
    "        {'name': 'BERT', 'year': 2018, 'params': 0.340, 'company': 'Google'},\n",
    "        {'name': 'GPT-2', 'year': 2019, 'params': 1.5, 'company': 'OpenAI'},\n",
    "        {'name': 'T5', 'year': 2019, 'params': 11, 'company': 'Google'},\n",
    "        {'name': 'GPT-3', 'year': 2020, 'params': 175, 'company': 'OpenAI'},\n",
    "        {'name': 'PaLM', 'year': 2022, 'params': 540, 'company': 'Google'},\n",
    "        {'name': 'LLaMA', 'year': 2023, 'params': 65, 'company': 'Meta'},\n",
    "        {'name': 'GPT-4', 'year': 2023, 'params': 1000, 'company': 'OpenAI'},  # 추정치\n",
    "        {'name': 'Claude', 'year': 2023, 'params': 100, 'company': 'Anthropic'},  # 추정치\n",
    "    ]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 1. 시간에 따른 모델 크기 변화\n",
    "    years = [m['year'] for m in models]\n",
    "    params = [m['params'] for m in models]\n",
    "    names = [m['name'] for m in models]\n",
    "    \n",
    "    ax1.scatter(years, params, s=100, alpha=0.6)\n",
    "    for i, name in enumerate(names):\n",
    "        ax1.annotate(name, (years[i], params[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Parameters (Billions)')\n",
    "    ax1.set_title('Evolution of LLM Model Sizes')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 회사별 모델 분포\n",
    "    companies = {}\n",
    "    for m in models:\n",
    "        company = m['company']\n",
    "        if company not in companies:\n",
    "            companies[company] = []\n",
    "        companies[company].append(m['name'])\n",
    "    \n",
    "    company_names = list(companies.keys())\n",
    "    model_counts = [len(companies[c]) for c in company_names]\n",
    "    \n",
    "    colors = plt.cm.Set3(range(len(company_names)))\n",
    "    ax2.pie(model_counts, labels=company_names, colors=colors, autopct='%1.1f%%')\n",
    "    ax2.set_title('LLM Models by Company')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 주요 아키텍처 설명\n",
    "    print(\"\\n=== 주요 LLM 아키텍처 ===\")\n",
    "    print(\"\\n1. **Encoder-only (BERT 계열)**:\")\n",
    "    print(\"   - 양방향 문맥 이해\")\n",
    "    print(\"   - 주로 분류, 추출 작업에 사용\")\n",
    "    print(\"   - 예: BERT, RoBERTa, ELECTRA\")\n",
    "    \n",
    "    print(\"\\n2. **Decoder-only (GPT 계열)**:\")\n",
    "    print(\"   - 자기회귀적 생성\")\n",
    "    print(\"   - 텍스트 생성에 특화\")\n",
    "    print(\"   - 예: GPT, GPT-2, GPT-3, LLaMA\")\n",
    "    \n",
    "    print(\"\\n3. **Encoder-Decoder (T5 계열)**:\")\n",
    "    print(\"   - 모든 작업을 텍스트 생성으로 통일\")\n",
    "    print(\"   - 번역, 요약 등에 효과적\")\n",
    "    print(\"   - 예: T5, BART, mT5\")\n",
    "\n",
    "visualize_llm_evolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hugging Face Transformers 기초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습된 모델 불러오기\n",
    "def load_pretrained_model(model_name=\"gpt2\"):\n",
    "    \"\"\"사전 학습된 모델과 토크나이저 로드\"\"\"\n",
    "    print(f\"모델 로딩 중: {model_name}\")\n",
    "    \n",
    "    # 토크나이저와 모델 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # 모델 정보\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"모델 로드 완료!\")\n",
    "    print(f\"총 파라미터 수: {total_params:,}\")\n",
    "    print(f\"모델 크기: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "# GPT-2 모델 로드\n",
    "tokenizer, model = load_pretrained_model(\"gpt2\")\n",
    "\n",
    "# 간단한 텍스트 생성\n",
    "def generate_text(prompt, model, tokenizer, max_length=50):\n",
    "    \"\"\"텍스트 생성 함수\"\"\"\n",
    "    # 입력 인코딩\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 디코딩\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# 텍스트 생성 예시\n",
    "prompt = \"Deep learning is\"\n",
    "generated = generate_text(prompt, model, tokenizer)\n",
    "print(f\"\\n프롬프트: '{prompt}'\")\n",
    "print(f\"생성된 텍스트: '{generated}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline API 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 작업을 위한 파이프라인\n",
    "def demonstrate_pipelines():\n",
    "    \"\"\"다양한 NLP 작업 파이프라인 시연\"\"\"\n",
    "    \n",
    "    print(\"=== 1. 텍스트 생성 ===\")\n",
    "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    result = generator(\n",
    "        \"The future of AI is\",\n",
    "        max_length=50,\n",
    "        num_return_sequences=2\n",
    "    )\n",
    "    for i, text in enumerate(result):\n",
    "        print(f\"생성 {i+1}: {text['generated_text']}\")\n",
    "    \n",
    "    print(\"\\n=== 2. 감성 분석 ===\")\n",
    "    sentiment = pipeline(\"sentiment-analysis\")\n",
    "    texts = [\n",
    "        \"I love this new deep learning framework!\",\n",
    "        \"This is terrible and doesn't work at all.\",\n",
    "        \"The performance is okay, nothing special.\"\n",
    "    ]\n",
    "    results = sentiment(texts)\n",
    "    for text, result in zip(texts, results):\n",
    "        print(f\"텍스트: '{text}'\")\n",
    "        print(f\"결과: {result['label']} (신뢰도: {result['score']:.3f})\\n\")\n",
    "    \n",
    "    print(\"=== 3. 질문 답변 ===\")\n",
    "    qa = pipeline(\"question-answering\")\n",
    "    context = \"\"\"Deep learning is a subset of machine learning that uses \n",
    "    artificial neural networks with multiple layers. It was inspired by \n",
    "    the structure and function of the human brain.\"\"\"\n",
    "    \n",
    "    questions = [\n",
    "        \"What is deep learning?\",\n",
    "        \"What inspired deep learning?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        result = qa(question=question, context=context)\n",
    "        print(f\"질문: {question}\")\n",
    "        print(f\"답변: {result['answer']} (신뢰도: {result['score']:.3f})\\n\")\n",
    "    \n",
    "    print(\"=== 4. 요약 ===\")\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    article = \"\"\"The Transformer architecture has revolutionized natural language processing. \n",
    "    Introduced in the paper 'Attention is All You Need', it replaced recurrent layers with \n",
    "    self-attention mechanisms. This allows for better parallelization and capturing of \n",
    "    long-range dependencies. The architecture consists of an encoder and decoder, each \n",
    "    with multiple layers of multi-head attention and feed-forward networks.\"\"\"\n",
    "    \n",
    "    summary = summarizer(article, max_length=50, min_length=20)\n",
    "    print(f\"원문 길이: {len(article.split())} 단어\")\n",
    "    print(f\"요약: {summary[0]['summary_text']}\")\n",
    "    print(f\"요약 길이: {len(summary[0]['summary_text'].split())} 단어\")\n",
    "\n",
    "demonstrate_pipelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptEngineering:\n",
    "    \"\"\"프롬프트 엔지니어링 기법들\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def zero_shot(model, tokenizer, task_description, input_text):\n",
    "        \"\"\"Zero-shot 프롬프팅\"\"\"\n",
    "        prompt = f\"{task_description}\\n\\nInput: {input_text}\\nOutput:\"\n",
    "        return generate_text(prompt, model, tokenizer, max_length=100)\n",
    "    \n",
    "    @staticmethod\n",
    "    def few_shot(model, tokenizer, examples, input_text):\n",
    "        \"\"\"Few-shot 프롬프팅\"\"\"\n",
    "        prompt = \"\"\n",
    "        for ex in examples:\n",
    "            prompt += f\"Input: {ex['input']}\\nOutput: {ex['output']}\\n\\n\"\n",
    "        prompt += f\"Input: {input_text}\\nOutput:\"\n",
    "        return generate_text(prompt, model, tokenizer, max_length=100)\n",
    "    \n",
    "    @staticmethod\n",
    "    def chain_of_thought(model, tokenizer, question):\n",
    "        \"\"\"Chain-of-thought 프롬프팅\"\"\"\n",
    "        prompt = f\"\"\"Question: {question}\n",
    "Let's think step by step:\n",
    "1.\"\"\"\n",
    "        return generate_text(prompt, model, tokenizer, max_length=150)\n",
    "\n",
    "# 프롬프트 엔지니어링 예시\n",
    "pe = PromptEngineering()\n",
    "\n",
    "print(\"=== 1. Zero-shot 예시 ===\")\n",
    "result = pe.zero_shot(\n",
    "    model, tokenizer,\n",
    "    \"Translate English to French:\",\n",
    "    \"Hello, how are you?\"\n",
    ")\n",
    "print(result)\n",
    "\n",
    "print(\"\\n=== 2. Few-shot 예시 ===\")\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"positive\"},\n",
    "    {\"input\": \"sad\", \"output\": \"negative\"},\n",
    "    {\"input\": \"excited\", \"output\": \"positive\"}\n",
    "]\n",
    "result = pe.few_shot(model, tokenizer, examples, \"disappointed\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\n=== 3. Chain-of-thought 예시 ===\")\n",
    "result = pe.chain_of_thought(\n",
    "    model, tokenizer,\n",
    "    \"If a train travels 60 km/h for 2 hours, how far does it go?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning 기초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 Fine-tuning 예시 (감성 분석)\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_sentiment_dataset():\n",
    "    \"\"\"감성 분석 데이터셋 준비\"\"\"\n",
    "    # 예시 데이터\n",
    "    data = {\n",
    "        'text': [\n",
    "            \"This movie is fantastic! Best I've seen all year.\",\n",
    "            \"Terrible film. Complete waste of time.\",\n",
    "            \"Amazing performance by the lead actor.\",\n",
    "            \"Boring plot and poor character development.\",\n",
    "            \"A masterpiece of modern cinema.\",\n",
    "            \"I fell asleep halfway through.\"\n",
    "        ],\n",
    "        'label': [1, 0, 1, 0, 1, 0]  # 1: positive, 0: negative\n",
    "    }\n",
    "    \n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset\n",
    "\n",
    "# 데이터셋 토큰화\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples['text'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Fine-tuning 설정 시각화\n",
    "def visualize_finetuning_process():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. Fine-tuning vs Training from scratch\n",
    "    epochs = np.arange(0, 50)\n",
    "    from_scratch = 100 * np.exp(-epochs/20) + 10\n",
    "    fine_tuned = 30 * np.exp(-epochs/5) + 10\n",
    "    \n",
    "    axes[0, 0].plot(epochs, from_scratch, 'b-', label='From Scratch', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, fine_tuned, 'r-', label='Fine-tuned', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epochs')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Efficiency: Fine-tuning vs From Scratch')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 데이터 요구량\n",
    "    data_sizes = [10, 100, 1000, 10000, 100000]\n",
    "    scratch_acc = [0.5, 0.6, 0.75, 0.85, 0.92]\n",
    "    finetune_acc = [0.7, 0.82, 0.88, 0.91, 0.93]\n",
    "    \n",
    "    axes[0, 1].semilogx(data_sizes, scratch_acc, 'b-o', label='From Scratch', linewidth=2)\n",
    "    axes[0, 1].semilogx(data_sizes, finetune_acc, 'r-o', label='Fine-tuned', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Training Data Size')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Data Efficiency')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Fine-tuning 전략\n",
    "    strategies = ['Full\\nFine-tuning', 'Feature\\nExtraction', 'LoRA', 'Adapter', 'Prompt\\nTuning']\n",
    "    params_tuned = [100, 0, 0.1, 1, 0.01]  # % of parameters tuned\n",
    "    performance = [95, 85, 92, 90, 88]\n",
    "    \n",
    "    x = np.arange(len(strategies))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1 = axes[1, 0]\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, params_tuned, width, label='Parameters Tuned (%)', color='skyblue')\n",
    "    bars2 = ax2.bar(x + width/2, performance, width, label='Performance (%)', color='lightcoral')\n",
    "    \n",
    "    ax1.set_xlabel('Fine-tuning Strategy')\n",
    "    ax1.set_ylabel('Parameters Tuned (%)', color='skyblue')\n",
    "    ax2.set_ylabel('Performance (%)', color='lightcoral')\n",
    "    ax1.set_title('Fine-tuning Strategies Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(strategies)\n",
    "    ax1.tick_params(axis='y', labelcolor='skyblue')\n",
    "    ax2.tick_params(axis='y', labelcolor='lightcoral')\n",
    "    \n",
    "    # 4. 학습률 스케줄\n",
    "    steps = np.arange(0, 1000)\n",
    "    warmup_steps = 100\n",
    "    \n",
    "    # Linear warmup + cosine decay\n",
    "    lr = np.ones_like(steps, dtype=float)\n",
    "    lr[:warmup_steps] = steps[:warmup_steps] / warmup_steps\n",
    "    lr[warmup_steps:] = 0.5 * (1 + np.cos(np.pi * (steps[warmup_steps:] - warmup_steps) / (1000 - warmup_steps)))\n",
    "    \n",
    "    axes[1, 1].plot(steps, lr, 'g-', linewidth=2)\n",
    "    axes[1, 1].axvline(x=warmup_steps, color='r', linestyle='--', alpha=0.5, label='End of Warmup')\n",
    "    axes[1, 1].set_xlabel('Training Steps')\n",
    "    axes[1, 1].set_ylabel('Learning Rate (Relative)')\n",
    "    axes[1, 1].set_title('Learning Rate Schedule')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_finetuning_process()\n",
    "\n",
    "print(\"\\n=== Fine-tuning 팁 ===\")\n",
    "print(\"1. **학습률**: 사전학습보다 낮은 학습률 사용 (1e-5 ~ 1e-4)\")\n",
    "print(\"2. **Warmup**: 초기 학습률을 점진적으로 증가\")\n",
    "print(\"3. **레이어 동결**: 하위 레이어는 동결하고 상위 레이어만 학습\")\n",
    "print(\"4. **정규화**: Dropout, weight decay 활용\")\n",
    "print(\"5. **조기 종료**: Validation loss 모니터링\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 효율적인 LLM 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientLLM:\n",
    "    \"\"\"효율적인 LLM 사용 기법\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantization_demo():\n",
    "        \"\"\"양자화로 모델 크기 줄이기\"\"\"\n",
    "        print(\"=== 양자화 (Quantization) ===\")\n",
    "        print(\"원리: 가중치를 낮은 비트로 표현\")\n",
    "        print(\"- FP32 (32-bit) → INT8 (8-bit): 75% 메모리 절약\")\n",
    "        print(\"- FP32 → INT4 (4-bit): 87.5% 메모리 절약\")\n",
    "        print(\"- 성능 손실: 일반적으로 1-3%\")\n",
    "        \n",
    "        # 시각화\n",
    "        bit_widths = [32, 16, 8, 4]\n",
    "        memory_usage = [100, 50, 25, 12.5]\n",
    "        accuracy = [100, 99.5, 98, 95]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        ax1.bar(bit_widths, memory_usage, color='skyblue')\n",
    "        ax1.set_xlabel('Bit Width')\n",
    "        ax1.set_ylabel('Memory Usage (%)')\n",
    "        ax1.set_title('Memory Usage vs Bit Width')\n",
    "        ax1.set_xticks(bit_widths)\n",
    "        \n",
    "        ax2.plot(bit_widths, accuracy, 'ro-', linewidth=2, markersize=8)\n",
    "        ax2.set_xlabel('Bit Width')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.set_title('Accuracy vs Bit Width')\n",
    "        ax2.set_xticks(bit_widths)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def efficient_attention():\n",
    "        \"\"\"효율적인 Attention 메커니즘\"\"\"\n",
    "        print(\"\\n=== 효율적인 Attention ===\")\n",
    "        \n",
    "        methods = {\n",
    "            'Standard Attention': 'O(n²) 메모리, 모든 토큰 쌍 계산',\n",
    "            'Flash Attention': '메모리 효율적, IO-aware 알고리즘',\n",
    "            'Sparse Attention': '일부 토큰만 attend, O(n√n)',\n",
    "            'Linear Attention': 'Kernel trick 사용, O(n)',\n",
    "            'Local Attention': '윈도우 내에서만 attention'\n",
    "        }\n",
    "        \n",
    "        for method, description in methods.items():\n",
    "            print(f\"- {method}: {description}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def model_parallelism():\n",
    "        \"\"\"모델 병렬화 전략\"\"\"\n",
    "        print(\"\\n=== 모델 병렬화 ===\")\n",
    "        print(\"1. **Data Parallelism**: 데이터를 나누어 처리\")\n",
    "        print(\"2. **Model Parallelism**: 모델을 나누어 처리\")\n",
    "        print(\"3. **Pipeline Parallelism**: 레이어를 파이프라인으로\")\n",
    "        print(\"4. **Tensor Parallelism**: 텐서 연산을 분할\")\n",
    "\n",
    "# 효율적인 LLM 사용법 시연\n",
    "efficient = EfficientLLM()\n",
    "efficient.quantization_demo()\n",
    "efficient.efficient_attention()\n",
    "efficient.model_parallelism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM의 한계와 윤리적 고려사항"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discuss_llm_limitations():\n",
    "    \"\"\"LLM의 한계와 윤리적 이슈\"\"\"\n",
    "    \n",
    "    # 한계점 시각화\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Hallucination 비율\n",
    "    tasks = ['Facts', 'Math', 'Code', 'Creative', 'Common Sense']\n",
    "    hallucination_rates = [15, 25, 20, 5, 30]\n",
    "    \n",
    "    axes[0, 0].bar(tasks, hallucination_rates, color='coral')\n",
    "    axes[0, 0].set_ylabel('Hallucination Rate (%)')\n",
    "    axes[0, 0].set_title('Hallucination Rates by Task Type')\n",
    "    axes[0, 0].set_ylim(0, 40)\n",
    "    \n",
    "    # 2. 컨텍스트 길이 제한\n",
    "    models = ['GPT-2', 'GPT-3', 'GPT-4', 'Claude', 'Gemini']\n",
    "    context_lengths = [1024, 4096, 8192, 100000, 128000]\n",
    "    \n",
    "    axes[0, 1].bar(models, np.log10(context_lengths), color='skyblue')\n",
    "    axes[0, 1].set_ylabel('Log10(Context Length)')\n",
    "    axes[0, 1].set_title('Context Length Limitations')\n",
    "    \n",
    "    # Y축 라벨 수정\n",
    "    y_labels = [f'{10**i:,}' for i in range(3, 6)]\n",
    "    axes[0, 1].set_yticks(range(3, 6))\n",
    "    axes[0, 1].set_yticklabels(y_labels)\n",
    "    \n",
    "    # 3. 편향성 예시\n",
    "    bias_types = ['Gender', 'Race', 'Religion', 'Politics', 'Culture']\n",
    "    bias_scores = [0.7, 0.8, 0.6, 0.9, 0.75]\n",
    "    \n",
    "    axes[1, 0].barh(bias_types, bias_scores, color='lightgreen')\n",
    "    axes[1, 0].set_xlabel('Bias Score (0-1)')\n",
    "    axes[1, 0].set_title('Types of Bias in LLMs')\n",
    "    axes[1, 0].set_xlim(0, 1)\n",
    "    \n",
    "    # 4. 환경 영향\n",
    "    model_sizes = ['1B', '10B', '100B', '1T']\n",
    "    co2_emissions = [0.1, 10, 1000, 100000]  # kg CO2\n",
    "    \n",
    "    axes[1, 1].semilogy(model_sizes, co2_emissions, 'ro-', linewidth=2, markersize=8)\n",
    "    axes[1, 1].set_xlabel('Model Size (Parameters)')\n",
    "    axes[1, 1].set_ylabel('CO2 Emissions (kg)')\n",
    "    axes[1, 1].set_title('Environmental Impact of Training LLMs')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n=== LLM의 주요 한계 ===\")\n",
    "    print(\"\\n1. **Hallucination (환각)**:\")\n",
    "    print(\"   - 그럴듯하지만 잘못된 정보 생성\")\n",
    "    print(\"   - 출처나 근거 없는 주장\")\n",
    "    print(\"   - 해결책: 사실 확인, 출처 요구\")\n",
    "    \n",
    "    print(\"\\n2. **컨텍스트 제한**:\")\n",
    "    print(\"   - 긴 문서 처리의 한계\")\n",
    "    print(\"   - 대화 기록 손실\")\n",
    "    print(\"   - 해결책: 요약, 청킹, 검색 증강\")\n",
    "    \n",
    "    print(\"\\n3. **편향성**:\")\n",
    "    print(\"   - 학습 데이터의 편향 반영\")\n",
    "    print(\"   - 사회적 스테레오타입 강화\")\n",
    "    print(\"   - 해결책: 다양한 데이터, 편향 감지\")\n",
    "    \n",
    "    print(\"\\n4. **프라이버시**:\")\n",
    "    print(\"   - 학습 데이터 유출 가능성\")\n",
    "    print(\"   - 개인정보 노출 위험\")\n",
    "    print(\"   - 해결책: 차등 프라이버시, 데이터 필터링\")\n",
    "    \n",
    "    print(\"\\n5. **환경 영향**:\")\n",
    "    print(\"   - 막대한 전력 소비\")\n",
    "    print(\"   - 탄소 배출\")\n",
    "    print(\"   - 해결책: 효율적 아키텍처, 재생 에너지\")\n",
    "\n",
    "discuss_llm_limitations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 실제 응용 사례 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMApplications:\n",
    "    \"\"\"LLM을 활용한 실제 응용 사례\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def code_assistant(self, description):\n",
    "        \"\"\"코드 생성 도우미\"\"\"\n",
    "        prompt = f\"\"\"# Python function to {description}\n",
    "def\"\"\"\n",
    "        generated = generate_text(prompt, self.model, self.tokenizer, max_length=150)\n",
    "        return generated\n",
    "    \n",
    "    def creative_writing(self, genre, theme):\n",
    "        \"\"\"창의적 글쓰기\"\"\"\n",
    "        prompt = f\"\"\"Write a short {genre} story about {theme}:\n",
    "\n",
    "Once upon a time,\"\"\"\n",
    "        generated = generate_text(prompt, self.model, self.tokenizer, max_length=200)\n",
    "        return generated\n",
    "    \n",
    "    def data_analysis_helper(self, data_description):\n",
    "        \"\"\"데이터 분석 도우미\"\"\"\n",
    "        prompt = f\"\"\"Given the following data: {data_description}\n",
    "        \n",
    "Analysis steps:\n",
    "1. First, we should\"\"\"\n",
    "        generated = generate_text(prompt, self.model, self.tokenizer, max_length=150)\n",
    "        return generated\n",
    "    \n",
    "    def educational_tutor(self, subject, question):\n",
    "        \"\"\"교육 도우미\"\"\"\n",
    "        prompt = f\"\"\"As a {subject} tutor, explain: {question}\n",
    "        \n",
    "Explanation:\"\"\"\n",
    "        generated = generate_text(prompt, self.model, self.tokenizer, max_length=200)\n",
    "        return generated\n",
    "\n",
    "# 응용 사례 실행\n",
    "apps = LLMApplications(model, tokenizer)\n",
    "\n",
    "print(\"=== 1. 코드 생성 도우미 ===\")\n",
    "code = apps.code_assistant(\"calculate fibonacci numbers\")\n",
    "print(code)\n",
    "\n",
    "print(\"\\n=== 2. 창의적 글쓰기 ===\")\n",
    "story = apps.creative_writing(\"science fiction\", \"time travel\")\n",
    "print(story[:300] + \"...\")\n",
    "\n",
    "print(\"\\n=== 3. 데이터 분석 도우미 ===\")\n",
    "analysis = apps.data_analysis_helper(\"customer purchase data with timestamps and amounts\")\n",
    "print(analysis)\n",
    "\n",
    "print(\"\\n=== 4. 교육 도우미 ===\")\n",
    "explanation = apps.educational_tutor(\"physics\", \"how does gravity work\")\n",
    "print(explanation[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LLM의 미래"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_llm_future():\n",
    "    \"\"\"LLM의 미래 전망 시각화\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. 모델 크기 예측\n",
    "    years = np.array([2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025])\n",
    "    model_sizes = np.array([0.1, 1.5, 175, 280, 540, 1000, 2000, 5000])  # Billions\n",
    "    \n",
    "    axes[0, 0].semilogy(years[:6], model_sizes[:6], 'bo-', label='Historical', linewidth=2)\n",
    "    axes[0, 0].semilogy(years[5:], model_sizes[5:], 'r--', label='Projected', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Year')\n",
    "    axes[0, 0].set_ylabel('Model Size (Billion Parameters)')\n",
    "    axes[0, 0].set_title('Growth of LLM Sizes')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 멀티모달 능력\n",
    "    modalities = ['Text', 'Image', 'Audio', 'Video', '3D']\n",
    "    current = [100, 80, 60, 30, 10]\n",
    "    future = [100, 95, 90, 80, 60]\n",
    "    \n",
    "    x = np.arange(len(modalities))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 1].bar(x - width/2, current, width, label='Current', color='lightblue')\n",
    "    axes[0, 1].bar(x + width/2, future, width, label='Future (2025)', color='lightgreen')\n",
    "    axes[0, 1].set_ylabel('Capability Level (%)')\n",
    "    axes[0, 1].set_title('Multimodal Capabilities')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(modalities)\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. 응용 분야\n",
    "    applications = [\n",
    "        'Healthcare', 'Education', 'Research', \n",
    "        'Entertainment', 'Business', 'Personal Assistant'\n",
    "    ]\n",
    "    adoption_rates = [40, 50, 70, 60, 80, 90]\n",
    "    \n",
    "    axes[1, 0].barh(applications, adoption_rates, color='coral')\n",
    "    axes[1, 0].set_xlabel('Adoption Rate (%)')\n",
    "    axes[1, 0].set_title('LLM Applications by 2025')\n",
    "    axes[1, 0].set_xlim(0, 100)\n",
    "    \n",
    "    # 4. 기술 발전 로드맵\n",
    "    timeline = ['2024', '2025', '2026', '2027', '2028']\n",
    "    milestones = [\n",
    "        'Efficient\\nArchitectures',\n",
    "        'Real-time\\nMultimodal',\n",
    "        'Personal\\nAI Agents',\n",
    "        'Scientific\\nDiscovery',\n",
    "        'AGI\\nCapabilities?'\n",
    "    ]\n",
    "    \n",
    "    axes[1, 1].scatter(timeline, range(len(timeline)), s=200, c='gold', edgecolors='black', linewidth=2)\n",
    "    for i, (time, milestone) in enumerate(zip(timeline, milestones)):\n",
    "        axes[1, 1].text(i, i + 0.1, milestone, ha='center', fontsize=10)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Year')\n",
    "    axes[1, 1].set_title('LLM Development Roadmap')\n",
    "    axes[1, 1].set_ylim(-0.5, len(timeline) - 0.5)\n",
    "    axes[1, 1].set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n=== LLM의 미래 전망 ===\")\n",
    "    print(\"\\n1. **더 효율적인 모델**:\")\n",
    "    print(\"   - Sparse models\")\n",
    "    print(\"   - Mixture of Experts\")\n",
    "    print(\"   - Neural Architecture Search\")\n",
    "    \n",
    "    print(\"\\n2. **멀티모달 통합**:\")\n",
    "    print(\"   - 텍스트 + 이미지 + 오디오 + 비디오\")\n",
    "    print(\"   - 실시간 처리\")\n",
    "    print(\"   - 3D 이해\")\n",
    "    \n",
    "    print(\"\\n3. **개인화된 AI**:\")\n",
    "    print(\"   - 개인 맞춤형 모델\")\n",
    "    print(\"   - 지속적 학습\")\n",
    "    print(\"   - 프라이버시 보장\")\n",
    "    \n",
    "    print(\"\\n4. **과학적 발견**:\")\n",
    "    print(\"   - 신약 개발\")\n",
    "    print(\"   - 재료 과학\")\n",
    "    print(\"   - 기후 모델링\")\n",
    "    \n",
    "    print(\"\\n5. **윤리적 AI**:\")\n",
    "    print(\"   - 설명 가능한 AI\")\n",
    "    print(\"   - 편향 제거\")\n",
    "    print(\"   - 안전한 AI\")\n",
    "\n",
    "visualize_llm_future()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 실습: 나만의 LLM 응용 프로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 챗봇 구현\n",
    "class SimpleChatbot:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        # 대화 기록에 추가\n",
    "        self.conversation_history.append(f\"User: {user_input}\")\n",
    "        \n",
    "        # 컨텍스트 생성 (최근 5개 대화만 유지)\n",
    "        context = \"\\n\".join(self.conversation_history[-5:])\n",
    "        prompt = f\"{context}\\nAssistant:\"\n",
    "        \n",
    "        # 응답 생성\n",
    "        response = generate_text(prompt, self.model, self.tokenizer, max_length=100)\n",
    "        \n",
    "        # 응답 추출\n",
    "        response_text = response.split(\"Assistant:\")[-1].strip()\n",
    "        if \"User:\" in response_text:\n",
    "            response_text = response_text.split(\"User:\")[0].strip()\n",
    "        \n",
    "        self.conversation_history.append(f\"Assistant: {response_text}\")\n",
    "        \n",
    "        return response_text\n",
    "    \n",
    "    def reset(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "# 챗봇 테스트\n",
    "chatbot = SimpleChatbot(model, tokenizer)\n",
    "\n",
    "print(\"=== 간단한 챗봇 데모 ===\")\n",
    "print(\"(입력 'quit'으로 종료)\\n\")\n",
    "\n",
    "# 미리 정의된 대화 예시\n",
    "test_conversations = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Can you explain it simply?\"\n",
    "]\n",
    "\n",
    "for user_input in test_conversations:\n",
    "    print(f\"You: {user_input}\")\n",
    "    response = chatbot.chat(user_input)\n",
    "    print(f\"Bot: {response}\\n\")\n",
    "\n",
    "print(\"\\n=== 대화 기록 ===\")\n",
    "for line in chatbot.conversation_history:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 1: RAG (Retrieval-Augmented Generation) 구현\n",
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    검색 증강 생성 구현\n",
    "    문서에서 관련 정보를 검색하고 LLM으로 답변 생성\n",
    "    \"\"\"\n",
    "    def __init__(self, documents, model, tokenizer):\n",
    "        self.documents = documents\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def retrieve(self, query, k=3):\n",
    "        # TODO: 쿼리와 가장 관련 있는 k개 문서 검색\n",
    "        # 힌트: 간단한 TF-IDF 또는 임베딩 유사도 사용\n",
    "        pass\n",
    "    \n",
    "    def generate_answer(self, query, retrieved_docs):\n",
    "        # TODO: 검색된 문서를 바탕으로 답변 생성\n",
    "        pass\n",
    "\n",
    "# 문제 2: 프롬프트 최적화\n",
    "def optimize_prompt(task, examples, constraints):\n",
    "    \"\"\"\n",
    "    주어진 작업에 대한 최적의 프롬프트 생성\n",
    "    \n",
    "    힌트:\n",
    "    - 명확한 지시사항\n",
    "    - 예시 포함\n",
    "    - 제약사항 명시\n",
    "    \"\"\"\n",
    "    # TODO: 구현하기\n",
    "    pass\n",
    "\n",
    "# 문제 3: 모델 평가 메트릭\n",
    "def evaluate_llm_output(generated_text, reference_text):\n",
    "    \"\"\"\n",
    "    LLM 출력 품질 평가\n",
    "    \n",
    "    평가 기준:\n",
    "    - 유창성 (Fluency)\n",
    "    - 일관성 (Coherence)\n",
    "    - 관련성 (Relevance)\n",
    "    - 정확성 (Accuracy)\n",
    "    \"\"\"\n",
    "    # TODO: 구현하기\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "이번 튜토리얼에서 배운 내용:\n",
    "1. **LLM의 발전**: GPT, BERT부터 최신 모델까지\n",
    "2. **Hugging Face 활용**: 사전학습 모델 사용법\n",
    "3. **Prompt Engineering**: 효과적인 프롬프트 작성\n",
    "4. **Fine-tuning**: 특정 작업에 맞게 조정\n",
    "5. **효율적 사용**: 양자화, 효율적 attention\n",
    "6. **한계와 윤리**: Hallucination, 편향성, 환경 영향\n",
    "7. **실제 응용**: 코드 생성, 창작, 교육 등\n",
    "\n",
    "### LLM 사용 시 기억할 점:\n",
    "- **검증**: 생성된 내용 항상 확인\n",
    "- **윤리**: 편향성과 해로운 콘텐츠 주의\n",
    "- **효율성**: 적절한 모델 크기 선택\n",
    "- **프라이버시**: 민감한 데이터 주의\n",
    "\n",
    "### 다음 학습 추천:\n",
    "1. **고급 기법**: LoRA, QLoRA, PEFT\n",
    "2. **멀티모달**: CLIP, DALL-E, Flamingo\n",
    "3. **특화 모델**: CodeLLM, BioLLM, LegalLLM\n",
    "4. **배포**: Model serving, API 구축\n",
    "\n",
    "축하합니다! 이제 여러분은 딥러닝의 기초부터 최신 LLM까지 전체적인 이해를 갖추었습니다! 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}