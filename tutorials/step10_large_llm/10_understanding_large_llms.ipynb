{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ì´í•´ì™€ í™œìš©\n",
    "\n",
    "ì´ì œ ì‹¤ì œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë“¤ì„ ì´í•´í•˜ê³  í™œìš©í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë´…ì‹œë‹¤.\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "1. í˜„ëŒ€ LLMì˜ ë°œì „ ê³¼ì •ê³¼ ì¢…ë¥˜\n",
    "2. Hugging Face Transformers ì‚¬ìš©ë²•\n",
    "3. Fine-tuningê³¼ Prompt Engineering\n",
    "4. LLMì˜ í•œê³„ì™€ ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­\n",
    "5. ì‹¤ì œ ì‘ìš© ì‚¬ë¡€ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì²˜ìŒ ì‹¤í–‰ ì‹œ)\n",
    "# !pip install transformers datasets accelerate sentencepiece\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LLMì˜ ë°œì „ ì—­ì‚¬ì™€ ì£¼ìš” ëª¨ë¸ë“¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_llm_evolution():\n",
    "    \"\"\"LLM ë°œì „ ê³¼ì • ì‹œê°í™”\"\"\"\n",
    "    \n",
    "    # ì£¼ìš” ëª¨ë¸ë“¤ì˜ ì •ë³´\n",
    "    models = [\n",
    "        {'name': 'GPT', 'year': 2018, 'params': 0.117, 'company': 'OpenAI'},\n",
    "        {'name': 'BERT', 'year': 2018, 'params': 0.340, 'company': 'Google'},\n",
    "        {'name': 'GPT-2', 'year': 2019, 'params': 1.5, 'company': 'OpenAI'},\n",
    "        {'name': 'T5', 'year': 2019, 'params': 11, 'company': 'Google'},\n",
    "        {'name': 'GPT-3', 'year': 2020, 'params': 175, 'company': 'OpenAI'},\n",
    "        {'name': 'PaLM', 'year': 2022, 'params': 540, 'company': 'Google'},\n",
    "        {'name': 'LLaMA', 'year': 2023, 'params': 65, 'company': 'Meta'},\n",
    "        {'name': 'GPT-4', 'year': 2023, 'params': 1000, 'company': 'OpenAI'},  # ì¶”ì •ì¹˜\n",
    "        {'name': 'Claude', 'year': 2023, 'params': 100, 'company': 'Anthropic'},  # ì¶”ì •ì¹˜\n",
    "    ]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 1. ì‹œê°„ì— ë”°ë¥¸ ëª¨ë¸ í¬ê¸° ë³€í™”\n",
    "    years = [m['year'] for m in models]\n",
    "    params = [m['params'] for m in models]\n",
    "    names = [m['name'] for m in models]\n",
    "    \n",
    "    ax1.scatter(years, params, s=100, alpha=0.6)\n",
    "    for i, name in enumerate(names):\n",
    "        ax1.annotate(name, (years[i], params[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Parameters (Billions)')\n",
    "    ax1.set_title('Evolution of LLM Model Sizes')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. íšŒì‚¬ë³„ ëª¨ë¸ ë¶„í¬\n",
    "    companies = {}\n",
    "    for m in models:\n",
    "        company = m['company']\n",
    "        if company not in companies:\n",
    "            companies[company] = []\n",
    "        companies[company].append(m['name'])\n",
    "    \n",
    "    company_names = list(companies.keys())\n",
    "    model_counts = [len(companies[c]) for c in company_names]\n",
    "    \n",
    "    colors = plt.cm.Set3(range(len(company_names)))\n",
    "    ax2.pie(model_counts, labels=company_names, colors=colors, autopct='%1.1f%%')\n",
    "    ax2.set_title('LLM Models by Company')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ì£¼ìš” ì•„í‚¤í…ì²˜ ì„¤ëª…\n",
    "    print(\"\\n=== ì£¼ìš” LLM ì•„í‚¤í…ì²˜ ===\")\n",
    "    print(\"\\n1. **Encoder-only (BERT ê³„ì—´)**:\")\n",
    "    print(\"   - ì–‘ë°©í–¥ ë¬¸ë§¥ ì´í•´\")\n",
    "    print(\"   - ì£¼ë¡œ ë¶„ë¥˜, ì¶”ì¶œ ì‘ì—…ì— ì‚¬ìš©\")\n",
    "    print(\"   - ì˜ˆ: BERT, RoBERTa, ELECTRA\")\n",
    "    \n",
    "    print(\"\\n2. **Decoder-only (GPT ê³„ì—´)**:\")\n",
    "    print(\"   - ìê¸°íšŒê·€ì  ìƒì„±\")\n",
    "    print(\"   - í…ìŠ¤íŠ¸ ìƒì„±ì— íŠ¹í™”\")\n",
    "    print(\"   - ì˜ˆ: GPT, GPT-2, GPT-3, LLaMA\")\n",
    "    \n",
    "    print(\"\\n3. **Encoder-Decoder (T5 ê³„ì—´)**:\")\n",
    "    print(\"   - ëª¨ë“  ì‘ì—…ì„ í…ìŠ¤íŠ¸ ìƒì„±ìœ¼ë¡œ í†µì¼\")\n",
    "    print(\"   - ë²ˆì—­, ìš”ì•½ ë“±ì— íš¨ê³¼ì \")\n",
    "    print(\"   - ì˜ˆ: T5, BART, mT5\")\n",
    "\n",
    "visualize_llm_evolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hugging Face Transformers ê¸°ì´ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "def load_pretrained_model(model_name=\"gpt2\"):\n",
    "    \"\"\"ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\"\"\"\n",
    "    print(f\"ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}\")\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # ëª¨ë¸ ì •ë³´\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    print(f\"ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}\")\n",
    "    print(f\"ëª¨ë¸ í¬ê¸°: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "# GPT-2 ëª¨ë¸ ë¡œë“œ\n",
    "tokenizer, model = load_pretrained_model(\"gpt2\")\n",
    "\n",
    "# ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "def generate_text(prompt, model, tokenizer, max_length=50):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜\"\"\"\n",
    "    # ì…ë ¥ ì¸ì½”ë”©\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # ìƒì„±\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # ë””ì½”ë”©\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì‹œ\n",
    "prompt = \"Deep learning is\"\n",
    "generated = generate_text(prompt, model, tokenizer)\n",
    "print(f\"\\ní”„ë¡¬í”„íŠ¸: '{prompt}'\")\n",
    "print(f\"ìƒì„±ëœ í…ìŠ¤íŠ¸: '{generated}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline API ì‚¬ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ì–‘í•œ ì‘ì—…ì„ ìœ„í•œ íŒŒì´í”„ë¼ì¸\n",
    "def demonstrate_pipelines():\n",
    "    \"\"\"ë‹¤ì–‘í•œ NLP ì‘ì—… íŒŒì´í”„ë¼ì¸ ì‹œì—°\"\"\"\n",
    "    \n",
    "    print(\"=== 1. í…ìŠ¤íŠ¸ ìƒì„± ===\")\n",
    "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    result = generator(\n",
    "        \"The future of AI is\",\n",
    "        max_length=50,\n",
    "        num_return_sequences=2\n",
    "    )\n",
    "    for i, text in enumerate(result):\n",
    "        print(f\"ìƒì„± {i+1}: {text['generated_text']}\")\n",
    "    \n",
    "    print(\"\\n=== 2. ê°ì„± ë¶„ì„ ===\")\n",
    "    sentiment = pipeline(\"sentiment-analysis\")\n",
    "    texts = [\n",
    "        \"I love this new deep learning framework!\",\n",
    "        \"This is terrible and doesn't work at all.\",\n",
    "        \"The performance is okay, nothing special.\"\n",
    "    ]\n",
    "    results = sentiment(texts)\n",
    "    for text, result in zip(texts, results):\n",
    "        print(f\"í…ìŠ¤íŠ¸: '{text}'\")\n",
    "        print(f\"ê²°ê³¼: {result['label']} (ì‹ ë¢°ë„: {result['score']:.3f})\\n\")\n",
    "    \n",
    "    print(\"=== 3. ì§ˆë¬¸ ë‹µë³€ ===\")\n",
    "    qa = pipeline(\"question-answering\")\n",
    "    context = \"\"\"Deep learning is a subset of machine learning that uses \n",
    "    artificial neural networks with multiple layers. It was inspired by \n",
    "    the structure and function of the human brain.\"\"\"\n",
    "    \n",
    "    questions = [\n",
    "        \"What is deep learning?\",\n",
    "        \"What inspired deep learning?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        result = qa(question=question, context=context)\n",
    "        print(f\"ì§ˆë¬¸: {question}\")\n",
    "        print(f\"ë‹µë³€: {result['answer']} (ì‹ ë¢°ë„: {result['score']:.3f})\\n\")\n",
    "    \n",
    "    print(\"=== 4. ìš”ì•½ ===\")\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    article = \"\"\"The Transformer architecture has revolutionized natural language processing. \n",
    "    Introduced in the paper 'Attention is All You Need', it replaced recurrent layers with \n",
    "    self-attention mechanisms. This allows for better parallelization and capturing of \n",
    "    long-range dependencies. The architecture consists of an encoder and decoder, each \n",
    "    with multiple layers of multi-head attention and feed-forward networks.\"\"\"\n",
    "    \n",
    "    summary = summarizer(article, max_length=50, min_length=20)\n",
    "    print(f\"ì›ë¬¸ ê¸¸ì´: {len(article.split())} ë‹¨ì–´\")\n",
    "    print(f\"ìš”ì•½: {summary[0]['summary_text']}\")\n",
    "    print(f\"ìš”ì•½ ê¸¸ì´: {len(summary[0]['summary_text'].split())} ë‹¨ì–´\")\n",
    "\n",
    "demonstrate_pipelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptEngineering:\n",
    "    \"\"\"í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²•ë“¤\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def zero_shot(model, tokenizer, task_description, input_text):\n",
    "        \"\"\"Zero-shot í”„ë¡¬í”„íŒ…\"\"\"\n",
    "        prompt = f\"{task_description}\\n\\nInput: {input_text}\\nOutput:\"\n",
    "        return generate_text(prompt, model, tokenizer, max_length=100)\n",
    "    \n",
    "    @staticmethod\n",
    "    def few_shot(model, tokenizer, examples, input_text):\n",
    "        \"\"\"Few-shot í”„ë¡¬í”„íŒ…\"\"\"\n",
    "        prompt = \"\"\n",
    "        for ex in examples:\n",
    "            prompt += f\"Input: {ex['input']}\\nOutput: {ex['output']}\\n\\n\"\n",
    "        prompt += f\"Input: {input_text}\\nOutput:\"\n",
    "        return generate_text(prompt, model, tokenizer, max_length=100)\n",
    "    \n",
    "    @staticmethod\n",
    "    def chain_of_thought(model, tokenizer, question):\n",
    "        \"\"\"Chain-of-thought í”„ë¡¬í”„íŒ…\"\"\"\n",
    "        prompt = f\"\"\"Question: {question}\n",
    "Let's think step by step:\n",
    "1.\"\"\"\n",
    "        return generate_text(prompt, model, tokenizer, max_length=150)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì˜ˆì‹œ\n",
    "pe = PromptEngineering()\n",
    "\n",
    "print(\"=== 1. Zero-shot ì˜ˆì‹œ ===\")\n",
    "result = pe.zero_shot(\n",
    "    model, tokenizer,\n",
    "    \"Translate English to French:\",\n",
    "    \"Hello, how are you?\"\n",
    ")\n",
    "print(result)\n",
    "\n",
    "print(\"\\n=== 2. Few-shot ì˜ˆì‹œ ===\")\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"positive\"},\n",
    "    {\"input\": \"sad\", \"output\": \"negative\"},\n",
    "    {\"input\": \"excited\", \"output\": \"positive\"}\n",
    "]\n",
    "result = pe.few_shot(model, tokenizer, examples, \"disappointed\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\n=== 3. Chain-of-thought ì˜ˆì‹œ ===\")\n",
    "result = pe.chain_of_thought(\n",
    "    model, tokenizer,\n",
    "    \"If a train travels 60 km/h for 2 hours, how far does it go?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning ê¸°ì´ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ Fine-tuning ì˜ˆì‹œ (ê°ì„± ë¶„ì„)\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_sentiment_dataset():\n",
    "    \"\"\"ê°ì„± ë¶„ì„ ë°ì´í„°ì…‹ ì¤€ë¹„\"\"\"\n",
    "    # ì˜ˆì‹œ ë°ì´í„°\n",
    "    data = {\n",
    "        'text': [\n",
    "            \"This movie is fantastic! Best I've seen all year.\",\n",
    "            \"Terrible film. Complete waste of time.\",\n",
    "            \"Amazing performance by the lead actor.\",\n",
    "            \"Boring plot and poor character development.\",\n",
    "            \"A masterpiece of modern cinema.\",\n",
    "            \"I fell asleep halfway through.\"\n",
    "        ],\n",
    "        'label': [1, 0, 1, 0, 1, 0]  # 1: positive, 0: negative\n",
    "    }\n",
    "    \n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset\n",
    "\n",
    "# ë°ì´í„°ì…‹ í† í°í™”\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples['text'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Fine-tuning ì„¤ì • ì‹œê°í™”\n",
    "def visualize_finetuning_process():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. Fine-tuning vs Training from scratch\n",
    "    epochs = np.arange(0, 50)\n",
    "    from_scratch = 100 * np.exp(-epochs/20) + 10\n",
    "    fine_tuned = 30 * np.exp(-epochs/5) + 10\n",
    "    \n",
    "    axes[0, 0].plot(epochs, from_scratch, 'b-', label='From Scratch', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, fine_tuned, 'r-', label='Fine-tuned', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epochs')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Efficiency: Fine-tuning vs From Scratch')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. ë°ì´í„° ìš”êµ¬ëŸ‰\n",
    "    data_sizes = [10, 100, 1000, 10000, 100000]\n",
    "    scratch_acc = [0.5, 0.6, 0.75, 0.85, 0.92]\n",
    "    finetune_acc = [0.7, 0.82, 0.88, 0.91, 0.93]\n",
    "    \n",
    "    axes[0, 1].semilogx(data_sizes, scratch_acc, 'b-o', label='From Scratch', linewidth=2)\n",
    "    axes[0, 1].semilogx(data_sizes, finetune_acc, 'r-o', label='Fine-tuned', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Training Data Size')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Data Efficiency')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Fine-tuning ì „ëµ\n",
    "    strategies = ['Full\\nFine-tuning', 'Feature\\nExtraction', 'LoRA', 'Adapter', 'Prompt\\nTuning']\n",
    "    params_tuned = [100, 0, 0.1, 1, 0.01]  # % of parameters tuned\n",
    "    performance = [95, 85, 92, 90, 88]\n",
    "    \n",
    "    x = np.arange(len(strategies))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1 = axes[1, 0]\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, params_tuned, width, label='Parameters Tuned (%)', color='skyblue')\n",
    "    bars2 = ax2.bar(x + width/2, performance, width, label='Performance (%)', color='lightcoral')\n",
    "    \n",
    "    ax1.set_xlabel('Fine-tuning Strategy')\n",
    "    ax1.set_ylabel('Parameters Tuned (%)', color='skyblue')\n",
    "    ax2.set_ylabel('Performance (%)', color='lightcoral')\n",
    "    ax1.set_title('Fine-tuning Strategies Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(strategies)\n",
    "    ax1.tick_params(axis='y', labelcolor='skyblue')\n",
    "    ax2.tick_params(axis='y', labelcolor='lightcoral')\n",
    "    \n",
    "    # 4. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„\n",
    "    steps = np.arange(0, 1000)\n",
    "    warmup_steps = 100\n",
    "    \n",
    "    # Linear warmup + cosine decay\n",
    "    lr = np.ones_like(steps, dtype=float)\n",
    "    lr[:warmup_steps] = steps[:warmup_steps] / warmup_steps\n",
    "    lr[warmup_steps:] = 0.5 * (1 + np.cos(np.pi * (steps[warmup_steps:] - warmup_steps) / (1000 - warmup_steps)))\n",
    "    \n",
    "    axes[1, 1].plot(steps, lr, 'g-', linewidth=2)\n",
    "    axes[1, 1].axvline(x=warmup_steps, color='r', linestyle='--', alpha=0.5, label='End of Warmup')\n",
    "    axes[1, 1].set_xlabel('Training Steps')\n",
    "    axes[1, 1].set_ylabel('Learning Rate (Relative)')\n",
    "    axes[1, 1].set_title('Learning Rate Schedule')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_finetuning_process()\n",
    "\n",
    "print(\"\\n=== Fine-tuning íŒ ===\")\n",
    "print(\"1. **í•™ìŠµë¥ **: ì‚¬ì „í•™ìŠµë³´ë‹¤ ë‚®ì€ í•™ìŠµë¥  ì‚¬ìš© (1e-5 ~ 1e-4)\")\n",
    "print(\"2. **Warmup**: ì´ˆê¸° í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ì¦ê°€\")\n",
    "print(\"3. **ë ˆì´ì–´ ë™ê²°**: í•˜ìœ„ ë ˆì´ì–´ëŠ” ë™ê²°í•˜ê³  ìƒìœ„ ë ˆì´ì–´ë§Œ í•™ìŠµ\")\n",
    "print(\"4. **ì •ê·œí™”**: Dropout, weight decay í™œìš©\")\n",
    "print(\"5. **ì¡°ê¸° ì¢…ë£Œ**: Validation loss ëª¨ë‹ˆí„°ë§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. íš¨ìœ¨ì ì¸ LLM ì‚¬ìš©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientLLM:\n",
    "    \"\"\"íš¨ìœ¨ì ì¸ LLM ì‚¬ìš© ê¸°ë²•\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantization_demo():\n",
    "        \"\"\"ì–‘ìí™”ë¡œ ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸°\"\"\"\n",
    "        print(\"=== ì–‘ìí™” (Quantization) ===\")\n",
    "        print(\"ì›ë¦¬: ê°€ì¤‘ì¹˜ë¥¼ ë‚®ì€ ë¹„íŠ¸ë¡œ í‘œí˜„\")\n",
    "        print(\"- FP32 (32-bit) â†’ INT8 (8-bit): 75% ë©”ëª¨ë¦¬ ì ˆì•½\")\n",
    "        print(\"- FP32 â†’ INT4 (4-bit): 87.5% ë©”ëª¨ë¦¬ ì ˆì•½\")\n",
    "        print(\"- ì„±ëŠ¥ ì†ì‹¤: ì¼ë°˜ì ìœ¼ë¡œ 1-3%\")\n",
    "        \n",
    "        # ì‹œê°í™”\n",
    "        bit_widths = [32, 16, 8, 4]\n",
    "        memory_usage = [100, 50, 25, 12.5]\n",
    "        accuracy = [100, 99.5, 98, 95]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        ax1.bar(bit_widths, memory_usage, color='skyblue')\n",
    "        ax1.set_xlabel('Bit Width')\n",
    "        ax1.set_ylabel('Memory Usage (%)')\n",
    "        ax1.set_title('Memory Usage vs Bit Width')\n",
    "        ax1.set_xticks(bit_widths)\n",
    "        \n",
    "        ax2.plot(bit_widths, accuracy, 'ro-', linewidth=2, markersize=8)\n",
    "        ax2.set_xlabel('Bit Width')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.set_title('Accuracy vs Bit Width')\n",
    "        ax2.set_xticks(bit_widths)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def efficient_attention():\n",
    "        \"\"\"íš¨ìœ¨ì ì¸ Attention ë©”ì»¤ë‹ˆì¦˜\"\"\"\n",
    "        print(\"\\n=== íš¨ìœ¨ì ì¸ Attention ===\")\n",
    "        \n",
    "        methods = {\n",
    "            'Standard Attention': 'O(nÂ²) ë©”ëª¨ë¦¬, ëª¨ë“  í† í° ìŒ ê³„ì‚°',\n",
    "            'Flash Attention': 'ë©”ëª¨ë¦¬ íš¨ìœ¨ì , IO-aware ì•Œê³ ë¦¬ì¦˜',\n",
    "            'Sparse Attention': 'ì¼ë¶€ í† í°ë§Œ attend, O(nâˆšn)',\n",
    "            'Linear Attention': 'Kernel trick ì‚¬ìš©, O(n)',\n",
    "            'Local Attention': 'ìœˆë„ìš° ë‚´ì—ì„œë§Œ attention'\n",
    "        }\n",
    "        \n",
    "        for method, description in methods.items():\n",
    "            print(f\"- {method}: {description}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def model_parallelism():\n",
    "        \"\"\"ëª¨ë¸ ë³‘ë ¬í™” ì „ëµ\"\"\"\n",
    "        print(\"\\n=== ëª¨ë¸ ë³‘ë ¬í™” ===\")\n",
    "        print(\"1. **Data Parallelism**: ë°ì´í„°ë¥¼ ë‚˜ëˆ„ì–´ ì²˜ë¦¬\")\n",
    "        print(\"2. **Model Parallelism**: ëª¨ë¸ì„ ë‚˜ëˆ„ì–´ ì²˜ë¦¬\")\n",
    "        print(\"3. **Pipeline Parallelism**: ë ˆì´ì–´ë¥¼ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ\")\n",
    "        print(\"4. **Tensor Parallelism**: í…ì„œ ì—°ì‚°ì„ ë¶„í• \")\n",
    "\n",
    "# íš¨ìœ¨ì ì¸ LLM ì‚¬ìš©ë²• ì‹œì—°\n",
    "efficient = EfficientLLM()\n",
    "efficient.quantization_demo()\n",
    "efficient.efficient_attention()\n",
    "efficient.model_parallelism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLMì˜ í•œê³„ì™€ ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discuss_llm_limitations():\n",
    "    \"\"\"LLMì˜ í•œê³„ì™€ ìœ¤ë¦¬ì  ì´ìŠˆ\"\"\"\n",
    "    \n",
    "    # í•œê³„ì  ì‹œê°í™”\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Hallucination ë¹„ìœ¨\n",
    "    tasks = ['Facts', 'Math', 'Code', 'Creative', 'Common Sense']\n",
    "    hallucination_rates = [15, 25, 20, 5, 30]\n",
    "    \n",
    "    axes[0, 0].bar(tasks, hallucination_rates, color='coral')\n",
    "    axes[0, 0].set_ylabel('Hallucination Rate (%)')\n",
    "    axes[0, 0].set_title('Hallucination Rates by Task Type')\n",
    "    axes[0, 0].set_ylim(0, 40)\n",
    "    \n",
    "    # 2. ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ\n",
    "    models = ['GPT-2', 'GPT-3', 'GPT-4', 'Claude', 'Gemini']\n",
    "    context_lengths = [1024, 4096, 8192, 100000, 128000]\n",
    "    \n",
    "    axes[0, 1].bar(models, np.log10(context_lengths), color='skyblue')\n",
    "    axes[0, 1].set_ylabel('Log10(Context Length)')\n",
    "    axes[0, 1].set_title('Context Length Limitations')\n",
    "    \n",
    "    # Yì¶• ë¼ë²¨ ìˆ˜ì •\n",
    "    y_labels = [f'{10**i:,}' for i in range(3, 6)]\n",
    "    axes[0, 1].set_yticks(range(3, 6))\n",
    "    axes[0, 1].set_yticklabels(y_labels)\n",
    "    \n",
    "    # 3. í¸í–¥ì„± ì˜ˆì‹œ\n",
    "    bias_types = ['Gender', 'Race', 'Religion', 'Politics', 'Culture']\n",
    "    bias_scores = [0.7, 0.8, 0.6, 0.9, 0.75]\n",
    "    \n",
    "    axes[1, 0].barh(bias_types, bias_scores, color='lightgreen')\n",
    "    axes[1, 0].set_xlabel('Bias Score (0-1)')\n",
    "    axes[1, 0].set_title('Types of Bias in LLMs')\n",
    "    axes[1, 0].set_xlim(0, 1)\n",
    "    \n",
    "    # 4. í™˜ê²½ ì˜í–¥\n",
    "    model_sizes = ['1B', '10B', '100B', '1T']\n",
    "    co2_emissions = [0.1, 10, 1000, 100000]  # kg CO2\n",
    "    \n",
    "    axes[1, 1].semilogy(model_sizes, co2_emissions, 'ro-', linewidth=2, markersize=8)\n",
    "    axes[1, 1].set_xlabel('Model Size (Parameters)')\n",
    "    axes[1, 1].set_ylabel('CO2 Emissions (kg)')\n",
    "    axes[1, 1].set_title('Environmental Impact of Training LLMs')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n=== LLMì˜ ì£¼ìš” í•œê³„ ===\")\n",
    "    print(\"\\n1. **Hallucination (í™˜ê°)**:\")\n",
    "    print(\"   - ê·¸ëŸ´ë“¯í•˜ì§€ë§Œ ì˜ëª»ëœ ì •ë³´ ìƒì„±\")\n",
    "    print(\"   - ì¶œì²˜ë‚˜ ê·¼ê±° ì—†ëŠ” ì£¼ì¥\")\n",
    "    print(\"   - í•´ê²°ì±…: ì‚¬ì‹¤ í™•ì¸, ì¶œì²˜ ìš”êµ¬\")\n",
    "    \n",
    "    print(\"\\n2. **ì»¨í…ìŠ¤íŠ¸ ì œí•œ**:\")\n",
    "    print(\"   - ê¸´ ë¬¸ì„œ ì²˜ë¦¬ì˜ í•œê³„\")\n",
    "    print(\"   - ëŒ€í™” ê¸°ë¡ ì†ì‹¤\")\n",
    "    print(\"   - í•´ê²°ì±…: ìš”ì•½, ì²­í‚¹, ê²€ìƒ‰ ì¦ê°•\")\n",
    "    \n",
    "    print(\"\\n3. **í¸í–¥ì„±**:\")\n",
    "    print(\"   - í•™ìŠµ ë°ì´í„°ì˜ í¸í–¥ ë°˜ì˜\")\n",
    "    print(\"   - ì‚¬íšŒì  ìŠ¤í…Œë ˆì˜¤íƒ€ì… ê°•í™”\")\n",
    "    print(\"   - í•´ê²°ì±…: ë‹¤ì–‘í•œ ë°ì´í„°, í¸í–¥ ê°ì§€\")\n",
    "    \n",
    "    print(\"\\n4. **í”„ë¼ì´ë²„ì‹œ**:\")\n",
    "    print(\"   - í•™ìŠµ ë°ì´í„° ìœ ì¶œ ê°€ëŠ¥ì„±\")\n",
    "    print(\"   - ê°œì¸ì •ë³´ ë…¸ì¶œ ìœ„í—˜\")\n",
    "    print(\"   - í•´ê²°ì±…: ì°¨ë“± í”„ë¼ì´ë²„ì‹œ, ë°ì´í„° í•„í„°ë§\")\n",
    "    \n",
    "    print(\"\\n5. **í™˜ê²½ ì˜í–¥**:\")\n",
    "    print(\"   - ë§‰ëŒ€í•œ ì „ë ¥ ì†Œë¹„\")\n",
    "    print(\"   - íƒ„ì†Œ ë°°ì¶œ\")\n",
    "    print(\"   - í•´ê²°ì±…: íš¨ìœ¨ì  ì•„í‚¤í…ì²˜, ì¬ìƒ ì—ë„ˆì§€\")\n",
    "\n",
    "discuss_llm_limitations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì‹¤ì œ ì‘ìš© ì‚¬ë¡€ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMApplications:\n",
    "    \"\"\"LLMì„ í™œìš©í•œ ì‹¤ì œ ì‘ìš© ì‚¬ë¡€\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def code_assistant(self, description):\n",
    "        \"\"\"ì½”ë“œ ìƒì„± ë„ìš°ë¯¸\"\"\"\n",
    "        prompt = f\"\"\"# Python function to {description}\n",
    "def\"\"\"\n",
    "        generated = generate_text(prompt, self.model, self.tokenizer, max_length=150)\n",
    "        return generated\n",
    "    \n",
    "    def creative_writing(self, genre, theme):\n",
    "        \"\"\"ì°½ì˜ì  ê¸€ì“°ê¸°\"\"\"\n",
    "        prompt = f\"\"\"Write a short {genre} story about {theme}:\n",
    "\n",
    "Once upon a time,\"\"\"\n",
    "        generated = generate_text(prompt, self.model, self.tokenizer, max_length=200)\n",
    "        return generated\n",
    "    \n",
    "    def data_analysis_helper(self, data_description):\n",
    "        \"\"\"ë°ì´í„° ë¶„ì„ ë„ìš°ë¯¸\"\"\"\n",
    "        prompt = f\"\"\"Given the following data: {data_description}\n",
    "        \n",
    "Analysis steps:\n",
    "1. First, we should\"\"\"\n",
    "        generated = generate_text(prompt, self.model, self.tokenizer, max_length=150)\n",
    "        return generated\n",
    "    \n",
    "    def educational_tutor(self, subject, question):\n",
    "        \"\"\"êµìœ¡ ë„ìš°ë¯¸\"\"\"\n",
    "        prompt = f\"\"\"As a {subject} tutor, explain: {question}\n",
    "        \n",
    "Explanation:\"\"\"\n",
    "        generated = generate_text(prompt, self.model, self.tokenizer, max_length=200)\n",
    "        return generated\n",
    "\n",
    "# ì‘ìš© ì‚¬ë¡€ ì‹¤í–‰\n",
    "apps = LLMApplications(model, tokenizer)\n",
    "\n",
    "print(\"=== 1. ì½”ë“œ ìƒì„± ë„ìš°ë¯¸ ===\")\n",
    "code = apps.code_assistant(\"calculate fibonacci numbers\")\n",
    "print(code)\n",
    "\n",
    "print(\"\\n=== 2. ì°½ì˜ì  ê¸€ì“°ê¸° ===\")\n",
    "story = apps.creative_writing(\"science fiction\", \"time travel\")\n",
    "print(story[:300] + \"...\")\n",
    "\n",
    "print(\"\\n=== 3. ë°ì´í„° ë¶„ì„ ë„ìš°ë¯¸ ===\")\n",
    "analysis = apps.data_analysis_helper(\"customer purchase data with timestamps and amounts\")\n",
    "print(analysis)\n",
    "\n",
    "print(\"\\n=== 4. êµìœ¡ ë„ìš°ë¯¸ ===\")\n",
    "explanation = apps.educational_tutor(\"physics\", \"how does gravity work\")\n",
    "print(explanation[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LLMì˜ ë¯¸ë˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_llm_future():\n",
    "    \"\"\"LLMì˜ ë¯¸ë˜ ì „ë§ ì‹œê°í™”\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. ëª¨ë¸ í¬ê¸° ì˜ˆì¸¡\n",
    "    years = np.array([2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025])\n",
    "    model_sizes = np.array([0.1, 1.5, 175, 280, 540, 1000, 2000, 5000])  # Billions\n",
    "    \n",
    "    axes[0, 0].semilogy(years[:6], model_sizes[:6], 'bo-', label='Historical', linewidth=2)\n",
    "    axes[0, 0].semilogy(years[5:], model_sizes[5:], 'r--', label='Projected', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Year')\n",
    "    axes[0, 0].set_ylabel('Model Size (Billion Parameters)')\n",
    "    axes[0, 0].set_title('Growth of LLM Sizes')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. ë©€í‹°ëª¨ë‹¬ ëŠ¥ë ¥\n",
    "    modalities = ['Text', 'Image', 'Audio', 'Video', '3D']\n",
    "    current = [100, 80, 60, 30, 10]\n",
    "    future = [100, 95, 90, 80, 60]\n",
    "    \n",
    "    x = np.arange(len(modalities))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 1].bar(x - width/2, current, width, label='Current', color='lightblue')\n",
    "    axes[0, 1].bar(x + width/2, future, width, label='Future (2025)', color='lightgreen')\n",
    "    axes[0, 1].set_ylabel('Capability Level (%)')\n",
    "    axes[0, 1].set_title('Multimodal Capabilities')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(modalities)\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. ì‘ìš© ë¶„ì•¼\n",
    "    applications = [\n",
    "        'Healthcare', 'Education', 'Research', \n",
    "        'Entertainment', 'Business', 'Personal Assistant'\n",
    "    ]\n",
    "    adoption_rates = [40, 50, 70, 60, 80, 90]\n",
    "    \n",
    "    axes[1, 0].barh(applications, adoption_rates, color='coral')\n",
    "    axes[1, 0].set_xlabel('Adoption Rate (%)')\n",
    "    axes[1, 0].set_title('LLM Applications by 2025')\n",
    "    axes[1, 0].set_xlim(0, 100)\n",
    "    \n",
    "    # 4. ê¸°ìˆ  ë°œì „ ë¡œë“œë§µ\n",
    "    timeline = ['2024', '2025', '2026', '2027', '2028']\n",
    "    milestones = [\n",
    "        'Efficient\\nArchitectures',\n",
    "        'Real-time\\nMultimodal',\n",
    "        'Personal\\nAI Agents',\n",
    "        'Scientific\\nDiscovery',\n",
    "        'AGI\\nCapabilities?'\n",
    "    ]\n",
    "    \n",
    "    axes[1, 1].scatter(timeline, range(len(timeline)), s=200, c='gold', edgecolors='black', linewidth=2)\n",
    "    for i, (time, milestone) in enumerate(zip(timeline, milestones)):\n",
    "        axes[1, 1].text(i, i + 0.1, milestone, ha='center', fontsize=10)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Year')\n",
    "    axes[1, 1].set_title('LLM Development Roadmap')\n",
    "    axes[1, 1].set_ylim(-0.5, len(timeline) - 0.5)\n",
    "    axes[1, 1].set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n=== LLMì˜ ë¯¸ë˜ ì „ë§ ===\")\n",
    "    print(\"\\n1. **ë” íš¨ìœ¨ì ì¸ ëª¨ë¸**:\")\n",
    "    print(\"   - Sparse models\")\n",
    "    print(\"   - Mixture of Experts\")\n",
    "    print(\"   - Neural Architecture Search\")\n",
    "    \n",
    "    print(\"\\n2. **ë©€í‹°ëª¨ë‹¬ í†µí•©**:\")\n",
    "    print(\"   - í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤ + ë¹„ë””ì˜¤\")\n",
    "    print(\"   - ì‹¤ì‹œê°„ ì²˜ë¦¬\")\n",
    "    print(\"   - 3D ì´í•´\")\n",
    "    \n",
    "    print(\"\\n3. **ê°œì¸í™”ëœ AI**:\")\n",
    "    print(\"   - ê°œì¸ ë§ì¶¤í˜• ëª¨ë¸\")\n",
    "    print(\"   - ì§€ì†ì  í•™ìŠµ\")\n",
    "    print(\"   - í”„ë¼ì´ë²„ì‹œ ë³´ì¥\")\n",
    "    \n",
    "    print(\"\\n4. **ê³¼í•™ì  ë°œê²¬**:\")\n",
    "    print(\"   - ì‹ ì•½ ê°œë°œ\")\n",
    "    print(\"   - ì¬ë£Œ ê³¼í•™\")\n",
    "    print(\"   - ê¸°í›„ ëª¨ë¸ë§\")\n",
    "    \n",
    "    print(\"\\n5. **ìœ¤ë¦¬ì  AI**:\")\n",
    "    print(\"   - ì„¤ëª… ê°€ëŠ¥í•œ AI\")\n",
    "    print(\"   - í¸í–¥ ì œê±°\")\n",
    "    print(\"   - ì•ˆì „í•œ AI\")\n",
    "\n",
    "visualize_llm_future()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ì‹¤ìŠµ: ë‚˜ë§Œì˜ LLM ì‘ìš© í”„ë¡œê·¸ë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ ì±—ë´‡ êµ¬í˜„\n",
    "class SimpleChatbot:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€\n",
    "        self.conversation_history.append(f\"User: {user_input}\")\n",
    "        \n",
    "        # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ìµœê·¼ 5ê°œ ëŒ€í™”ë§Œ ìœ ì§€)\n",
    "        context = \"\\n\".join(self.conversation_history[-5:])\n",
    "        prompt = f\"{context}\\nAssistant:\"\n",
    "        \n",
    "        # ì‘ë‹µ ìƒì„±\n",
    "        response = generate_text(prompt, self.model, self.tokenizer, max_length=100)\n",
    "        \n",
    "        # ì‘ë‹µ ì¶”ì¶œ\n",
    "        response_text = response.split(\"Assistant:\")[-1].strip()\n",
    "        if \"User:\" in response_text:\n",
    "            response_text = response_text.split(\"User:\")[0].strip()\n",
    "        \n",
    "        self.conversation_history.append(f\"Assistant: {response_text}\")\n",
    "        \n",
    "        return response_text\n",
    "    \n",
    "    def reset(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "# ì±—ë´‡ í…ŒìŠ¤íŠ¸\n",
    "chatbot = SimpleChatbot(model, tokenizer)\n",
    "\n",
    "print(\"=== ê°„ë‹¨í•œ ì±—ë´‡ ë°ëª¨ ===\")\n",
    "print(\"(ì…ë ¥ 'quit'ìœ¼ë¡œ ì¢…ë£Œ)\\n\")\n",
    "\n",
    "# ë¯¸ë¦¬ ì •ì˜ëœ ëŒ€í™” ì˜ˆì‹œ\n",
    "test_conversations = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Can you explain it simply?\"\n",
    "]\n",
    "\n",
    "for user_input in test_conversations:\n",
    "    print(f\"You: {user_input}\")\n",
    "    response = chatbot.chat(user_input)\n",
    "    print(f\"Bot: {response}\\n\")\n",
    "\n",
    "print(\"\\n=== ëŒ€í™” ê¸°ë¡ ===\")\n",
    "for line in chatbot.conversation_history:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì—°ìŠµ ë¬¸ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì œ 1: RAG (Retrieval-Augmented Generation) êµ¬í˜„\n",
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    ê²€ìƒ‰ ì¦ê°• ìƒì„± êµ¬í˜„\n",
    "    ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±\n",
    "    \"\"\"\n",
    "    def __init__(self, documents, model, tokenizer):\n",
    "        self.documents = documents\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def retrieve(self, query, k=3):\n",
    "        # TODO: ì¿¼ë¦¬ì™€ ê°€ì¥ ê´€ë ¨ ìˆëŠ” kê°œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "        # íŒíŠ¸: ê°„ë‹¨í•œ TF-IDF ë˜ëŠ” ì„ë² ë”© ìœ ì‚¬ë„ ì‚¬ìš©\n",
    "        pass\n",
    "    \n",
    "    def generate_answer(self, query, retrieved_docs):\n",
    "        # TODO: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±\n",
    "        pass\n",
    "\n",
    "# ë¬¸ì œ 2: í”„ë¡¬í”„íŠ¸ ìµœì í™”\n",
    "def optimize_prompt(task, examples, constraints):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ì‘ì—…ì— ëŒ€í•œ ìµœì ì˜ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    \n",
    "    íŒíŠ¸:\n",
    "    - ëª…í™•í•œ ì§€ì‹œì‚¬í•­\n",
    "    - ì˜ˆì‹œ í¬í•¨\n",
    "    - ì œì•½ì‚¬í•­ ëª…ì‹œ\n",
    "    \"\"\"\n",
    "    # TODO: êµ¬í˜„í•˜ê¸°\n",
    "    pass\n",
    "\n",
    "# ë¬¸ì œ 3: ëª¨ë¸ í‰ê°€ ë©”íŠ¸ë¦­\n",
    "def evaluate_llm_output(generated_text, reference_text):\n",
    "    \"\"\"\n",
    "    LLM ì¶œë ¥ í’ˆì§ˆ í‰ê°€\n",
    "    \n",
    "    í‰ê°€ ê¸°ì¤€:\n",
    "    - ìœ ì°½ì„± (Fluency)\n",
    "    - ì¼ê´€ì„± (Coherence)\n",
    "    - ê´€ë ¨ì„± (Relevance)\n",
    "    - ì •í™•ì„± (Accuracy)\n",
    "    \"\"\"\n",
    "    # TODO: êµ¬í˜„í•˜ê¸°\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì •ë¦¬\n",
    "\n",
    "ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œ ë°°ìš´ ë‚´ìš©:\n",
    "1. **LLMì˜ ë°œì „**: GPT, BERTë¶€í„° ìµœì‹  ëª¨ë¸ê¹Œì§€\n",
    "2. **Hugging Face í™œìš©**: ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì‚¬ìš©ë²•\n",
    "3. **Prompt Engineering**: íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ ì‘ì„±\n",
    "4. **Fine-tuning**: íŠ¹ì • ì‘ì—…ì— ë§ê²Œ ì¡°ì •\n",
    "5. **íš¨ìœ¨ì  ì‚¬ìš©**: ì–‘ìí™”, íš¨ìœ¨ì  attention\n",
    "6. **í•œê³„ì™€ ìœ¤ë¦¬**: Hallucination, í¸í–¥ì„±, í™˜ê²½ ì˜í–¥\n",
    "7. **ì‹¤ì œ ì‘ìš©**: ì½”ë“œ ìƒì„±, ì°½ì‘, êµìœ¡ ë“±\n",
    "\n",
    "### LLM ì‚¬ìš© ì‹œ ê¸°ì–µí•  ì :\n",
    "- **ê²€ì¦**: ìƒì„±ëœ ë‚´ìš© í•­ìƒ í™•ì¸\n",
    "- **ìœ¤ë¦¬**: í¸í–¥ì„±ê³¼ í•´ë¡œìš´ ì½˜í…ì¸  ì£¼ì˜\n",
    "- **íš¨ìœ¨ì„±**: ì ì ˆí•œ ëª¨ë¸ í¬ê¸° ì„ íƒ\n",
    "- **í”„ë¼ì´ë²„ì‹œ**: ë¯¼ê°í•œ ë°ì´í„° ì£¼ì˜\n",
    "\n",
    "### ë‹¤ìŒ í•™ìŠµ ì¶”ì²œ:\n",
    "1. **ê³ ê¸‰ ê¸°ë²•**: LoRA, QLoRA, PEFT\n",
    "2. **ë©€í‹°ëª¨ë‹¬**: CLIP, DALL-E, Flamingo\n",
    "3. **íŠ¹í™” ëª¨ë¸**: CodeLLM, BioLLM, LegalLLM\n",
    "4. **ë°°í¬**: Model serving, API êµ¬ì¶•\n",
    "\n",
    "ì¶•í•˜í•©ë‹ˆë‹¤! ì´ì œ ì—¬ëŸ¬ë¶„ì€ ë”¥ëŸ¬ë‹ì˜ ê¸°ì´ˆë¶€í„° ìµœì‹  LLMê¹Œì§€ ì „ì²´ì ì¸ ì´í•´ë¥¼ ê°–ì¶”ì—ˆìŠµë‹ˆë‹¤! ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}