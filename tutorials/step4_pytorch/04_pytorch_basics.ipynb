{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: PyTorch 기초 - 프레임워크로 딥러닝 시작하기\n",
    "\n",
    "지금까지 NumPy로 신경망을 직접 구현했습니다. 이제 PyTorch를 사용하여 더 효율적으로 딥러닝을 해봅시다!\n",
    "\n",
    "## 학습 목표\n",
    "1. PyTorch의 기본 개념 이해 (텐서, 자동 미분)\n",
    "2. nn.Module로 신경망 구축하기\n",
    "3. 옵티마이저와 손실 함수 사용하기\n",
    "4. GPU 활용하기\n",
    "5. 데이터로더로 효율적인 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 시각화 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 재현성을 위한 시드 설정\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch 텐서 (Tensor)\n",
    "\n",
    "PyTorch의 텐서는 NumPy 배열과 비슷하지만, GPU에서 연산이 가능하고 자동 미분을 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 생성\n",
    "# NumPy 배열에서 텐서 생성\n",
    "numpy_array = np.array([1, 2, 3, 4, 5])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(\"NumPy에서 생성:\", tensor_from_numpy)\n",
    "\n",
    "# 직접 텐서 생성\n",
    "tensor_direct = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "print(\"직접 생성:\", tensor_direct)\n",
    "\n",
    "# 특수 텐서들\n",
    "zeros = torch.zeros(3, 4)\n",
    "ones = torch.ones(2, 3)\n",
    "random = torch.randn(3, 3)  # 표준정규분포\n",
    "\n",
    "print(\"\\n영텐서 (3x4):\")\n",
    "print(zeros)\n",
    "print(\"\\n무작위 텐서 (3x3):\")\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 텐서 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 연산\n",
    "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "print(\"텐서 a:\")\n",
    "print(a)\n",
    "print(\"\\n텐서 b:\")\n",
    "print(b)\n",
    "\n",
    "# 요소별 연산\n",
    "print(\"\\n덧셈 (a + b):\")\n",
    "print(a + b)\n",
    "\n",
    "print(\"\\n곱셈 (a * b):\")\n",
    "print(a * b)\n",
    "\n",
    "# 행렬 곱셈\n",
    "print(\"\\n행렬 곱셈 (a @ b):\")\n",
    "print(a @ b)  # 또는 torch.matmul(a, b)\n",
    "\n",
    "# 통계 함수\n",
    "print(\"\\n평균:\", a.mean())\n",
    "print(\"표준편차:\", a.std())\n",
    "print(\"최댓값:\", a.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 자동 미분 (Autograd)\n",
    "\n",
    "PyTorch의 가장 강력한 기능 중 하나는 자동 미분입니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad=True로 미분 가능한 텐서 생성\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "\n",
    "# 연산 수행\n",
    "y = x**2 + 3*x + 1\n",
    "print(f\"y = x² + 3x + 1 = {y}\")\n",
    "\n",
    "# 역전파\n",
    "y.backward()\n",
    "\n",
    "# 미분값 확인 (dy/dx = 2x + 3)\n",
    "print(f\"\\ndy/dx at x=2: {x.grad}\")\n",
    "print(f\"예상값: {2*2 + 3} (2x + 3)\")\n",
    "\n",
    "# 더 복잡한 예제\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x.sum()\n",
    "z = y**2\n",
    "\n",
    "print(f\"\\nx = {x}\")\n",
    "print(f\"y = sum(x) = {y}\")\n",
    "print(f\"z = y² = {z}\")\n",
    "\n",
    "z.backward()\n",
    "print(f\"\\ndz/dx = {x.grad}\")\n",
    "print(\"(각 요소의 기여도가 2*sum(x) = 12)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. nn.Module로 신경망 구축하기\n",
    "\n",
    "이제 Step 3에서 만든 MLP를 PyTorch로 다시 구현해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        # 레이어 정의\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 첫 번째 완전연결층\n",
    "        self.relu = nn.ReLU()                          # ReLU 활성화 함수\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # 두 번째 완전연결층\n",
    "        self.sigmoid = nn.Sigmoid()                    # 시그모이드 활성화 함수\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 순전파 정의\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# 모델 생성\n",
    "model = SimpleMLP(input_size=2, hidden_size=4, output_size=1)\n",
    "print(\"모델 구조:\")\n",
    "print(model)\n",
    "\n",
    "# 파라미터 확인\n",
    "print(\"\\n모델 파라미터:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 더 간결한 방법: nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential을 사용한 동일한 모델\n",
    "model_seq = nn.Sequential(\n",
    "    nn.Linear(2, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "print(\"Sequential 모델:\")\n",
    "print(model_seq)\n",
    "\n",
    "# 더 복잡한 모델 예제\n",
    "complex_model = nn.Sequential(\n",
    "    nn.Linear(2, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),      # 드롭아웃 추가\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(8, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "print(\"\\n복잡한 모델 (드롭아웃 포함):\")\n",
    "print(complex_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XOR 문제 다시 해결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR 데이터 준비\n",
    "X_xor = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_xor = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저 정의\n",
    "model_xor = nn.Sequential(\n",
    "    nn.Linear(2, 4),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(4, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.SGD(model_xor.parameters(), lr=0.5)\n",
    "\n",
    "# 학습\n",
    "losses = []\n",
    "for epoch in range(1000):\n",
    "    # 순전파\n",
    "    outputs = model_xor(X_xor)\n",
    "    loss = criterion(outputs, y_xor)\n",
    "    \n",
    "    # 역전파\n",
    "    optimizer.zero_grad()  # 기울기 초기화\n",
    "    loss.backward()        # 역전파\n",
    "    optimizer.step()       # 가중치 업데이트\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 결과 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# 학습 곡선\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 결정 경계\n",
    "plt.subplot(1, 3, 2)\n",
    "x_min, x_max = -0.5, 1.5\n",
    "y_min, y_max = -0.5, 1.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "grid_tensor = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    Z = model_xor(grid_tensor).numpy()\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, levels=20, alpha=0.8, cmap='RdBu')\n",
    "plt.colorbar(label='Output')\n",
    "plt.scatter([0, 1], [0, 1], c='red', s=200, edgecolors='black', linewidths=2)\n",
    "plt.scatter([0, 1], [1, 0], c='blue', s=200, edgecolors='black', linewidths=2)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('XOR Decision Boundary')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 예측 결과\n",
    "plt.subplot(1, 3, 3)\n",
    "with torch.no_grad():\n",
    "    predictions = (model_xor(X_xor) > 0.5).float()\n",
    "    probs = model_xor(X_xor)\n",
    "\n",
    "table_data = [[x1, x2, y, pred.item(), f\"{prob.item():.3f}\"] \n",
    "              for (x1, x2), y, pred, prob in zip(X_xor.numpy(), y_xor.numpy(), predictions, probs)]\n",
    "plt.table(cellText=table_data,\n",
    "          colLabels=['x1', 'x2', 'Target', 'Prediction', 'Probability'],\n",
    "          cellLoc='center',\n",
    "          loc='center')\n",
    "plt.axis('off')\n",
    "plt.title('Predictions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPU 사용하기\n",
    "\n",
    "PyTorch의 큰 장점 중 하나는 간단하게 GPU를 사용할 수 있다는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 가능한 디바이스: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# 모델과 데이터를 GPU로 이동\n",
    "model_gpu = nn.Sequential(\n",
    "    nn.Linear(2, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 1),\n",
    "    nn.Sigmoid()\n",
    ").to(device)  # GPU로 이동\n",
    "\n",
    "# 데이터도 GPU로 이동\n",
    "X_gpu = X_xor.to(device)\n",
    "y_gpu = y_xor.to(device)\n",
    "\n",
    "print(f\"\\n모델 위치: {next(model_gpu.parameters()).device}\")\n",
    "print(f\"데이터 위치: {X_gpu.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 데이터셋과 데이터로더\n",
    "\n",
    "실제 딥러닝에서는 데이터를 효율적으로 관리하기 위해 Dataset과 DataLoader를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 데이터셋 클래스\n",
    "class MoonsDataset(Dataset):\n",
    "    def __init__(self, n_samples=1000, noise=0.2, train=True, test_size=0.2):\n",
    "        # 데이터 생성\n",
    "        X, y = make_moons(n_samples=n_samples, noise=noise, random_state=42)\n",
    "        X = (X - X.mean(axis=0)) / X.std(axis=0)  # 정규화\n",
    "        \n",
    "        # 학습/테스트 분할\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        if train:\n",
    "            self.X = torch.tensor(X_train, dtype=torch.float32)\n",
    "            self.y = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "        else:\n",
    "            self.X = torch.tensor(X_test, dtype=torch.float32)\n",
    "            self.y = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = MoonsDataset(n_samples=1000, train=True)\n",
    "test_dataset = MoonsDataset(n_samples=1000, train=False)\n",
    "\n",
    "print(f\"학습 데이터 크기: {len(train_dataset)}\")\n",
    "print(f\"테스트 데이터 크기: {len(test_dataset)}\")\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 배치 확인\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print(f\"\\n배치 {batch_idx}: 입력 형태 = {data.shape}, 타겟 형태 = {target.shape}\")\n",
    "    if batch_idx == 2:  # 처음 3개 배치만 출력\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 완전한 학습 파이프라인 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.2):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # 은닉층 구성\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),  # 배치 정규화\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # 출력층\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 학습 함수\n",
    "def train_model(model, train_loader, test_loader, epochs=50, learning_rate=0.01):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()  # Sigmoid + BCE Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 학습 모드\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pred = (torch.sigmoid(output) > 0.5).float()\n",
    "            train_correct += pred.eq(target).sum().item()\n",
    "        \n",
    "        # 평가 모드\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_correct = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += criterion(output, target).item()\n",
    "                pred = (torch.sigmoid(output) > 0.5).float()\n",
    "                test_correct += pred.eq(target).sum().item()\n",
    "        \n",
    "        # 평균 손실과 정확도 계산\n",
    "        train_loss /= len(train_loader)\n",
    "        test_loss /= len(test_loader)\n",
    "        train_acc = train_correct / len(train_loader.dataset)\n",
    "        test_acc = test_correct / len(test_loader.dataset)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        # 학습률 조정\n",
    "        scheduler.step(test_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}, \"\n",
    "                  f\"Test Loss = {test_loss:.4f}, Test Acc = {test_acc:.4f}\")\n",
    "    \n",
    "    return train_losses, test_losses, train_accs, test_accs\n",
    "\n",
    "# 모델 생성 및 학습\n",
    "model = NeuralNetwork(input_size=2, hidden_sizes=[16, 8], output_size=1)\n",
    "print(\"모델 구조:\")\n",
    "print(model)\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train_model(\n",
    "    model, train_loader, test_loader, epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과 시각화\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 손실 그래프\n",
    "axes[0, 0].plot(train_losses, label='Train Loss')\n",
    "axes[0, 0].plot(test_losses, label='Test Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Test Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 정확도 그래프\n",
    "axes[0, 1].plot(train_accs, label='Train Accuracy')\n",
    "axes[0, 1].plot(test_accs, label='Test Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Training and Test Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 결정 경계 시각화\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "# 전체 데이터 가져오기\n",
    "X_all = torch.cat([train_dataset.X, test_dataset.X])\n",
    "y_all = torch.cat([train_dataset.y, test_dataset.y])\n",
    "\n",
    "# 결정 경계 계산\n",
    "x_min, x_max = X_all[:, 0].min() - 0.5, X_all[:, 0].max() + 0.5\n",
    "y_min, y_max = X_all[:, 1].min() - 0.5, X_all[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "grid_tensor = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    Z = torch.sigmoid(model(grid_tensor)).cpu().numpy()\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axes[1, 0].contourf(xx, yy, Z, levels=20, alpha=0.8, cmap='RdBu')\n",
    "axes[1, 0].scatter(X_all[y_all.squeeze() == 0][:, 0], X_all[y_all.squeeze() == 0][:, 1], \n",
    "                   c='red', alpha=0.6, edgecolors='black', linewidths=1)\n",
    "axes[1, 0].scatter(X_all[y_all.squeeze() == 1][:, 0], X_all[y_all.squeeze() == 1][:, 1], \n",
    "                   c='blue', alpha=0.6, edgecolors='black', linewidths=1)\n",
    "axes[1, 0].set_xlabel('Feature 1')\n",
    "axes[1, 0].set_ylabel('Feature 2')\n",
    "axes[1, 0].set_title('Decision Boundary')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 예측 확률 분포\n",
    "with torch.no_grad():\n",
    "    probs = torch.sigmoid(model(X_all.to(device))).cpu().numpy()\n",
    "\n",
    "axes[1, 1].hist(probs[y_all.squeeze() == 0], bins=20, alpha=0.5, label='Class 0', color='red')\n",
    "axes[1, 1].hist(probs[y_all.squeeze() == 1], bins=20, alpha=0.5, label='Class 1', color='blue')\n",
    "axes[1, 1].set_xlabel('Predicted Probability')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('Prediction Probability Distribution')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 다양한 최적화 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 옵티마이저 비교\n",
    "optimizers = {\n",
    "    'SGD': optim.SGD,\n",
    "    'Adam': optim.Adam,\n",
    "    'RMSprop': optim.RMSprop,\n",
    "    'AdamW': optim.AdamW\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, optimizer_class) in enumerate(optimizers.items()):\n",
    "    # 모델 초기화\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(2, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 8),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # 학습\n",
    "    losses = []\n",
    "    for epoch in range(100):\n",
    "        epoch_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "    \n",
    "    axes[idx].plot(losses)\n",
    "    axes[idx].set_xlabel('Epoch')\n",
    "    axes[idx].set_ylabel('Loss')\n",
    "    axes[idx].set_title(f'{name} Optimizer')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 모델 저장과 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "# 방법 1: 전체 모델 저장\n",
    "torch.save(model, 'complete_model.pth')\n",
    "\n",
    "# 방법 2: 가중치만 저장 (권장)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "# 체크포인트 저장 (학습 재개용)\n",
    "checkpoint = {\n",
    "    'epoch': 50,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': train_losses[-1],\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n",
    "\n",
    "print(\"모델 저장 완료!\")\n",
    "\n",
    "# 모델 불러오기\n",
    "# 방법 1: 전체 모델 불러오기\n",
    "loaded_model = torch.load('complete_model.pth')\n",
    "\n",
    "# 방법 2: 가중치만 불러오기 (권장)\n",
    "new_model = NeuralNetwork(input_size=2, hidden_sizes=[16, 8], output_size=1)\n",
    "new_model.load_state_dict(torch.load('model_weights.pth'))\n",
    "new_model.eval()  # 평가 모드로 전환\n",
    "\n",
    "print(\"\\n모델 불러오기 완료!\")\n",
    "\n",
    "# 불러온 모델로 예측\n",
    "with torch.no_grad():\n",
    "    sample_data = torch.tensor([[0.5, 0.5]], dtype=torch.float32)\n",
    "    prediction = torch.sigmoid(new_model(sample_data))\n",
    "    print(f\"\\n샘플 예측: 입력 = {sample_data.numpy()}, 출력 = {prediction.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 실전 팁과 트릭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 가중치 초기화\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 1)\n",
    ")\n",
    "model.apply(init_weights)\n",
    "print(\"가중치 초기화 완료\")\n",
    "\n",
    "# 2. 그래디언트 클리핑\n",
    "def train_with_gradient_clipping(model, data_loader, epochs=10):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data, target in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # 그래디언트 클리핑\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            break  # 데모용으로 첫 배치만 실행\n",
    "\n",
    "# 3. 조기 종료 (Early Stopping)\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# 사용 예시\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "print(\"Early Stopping 준비 완료\")\n",
    "\n",
    "# 4. 학습률 스케줄링\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 다양한 스케줄러\n",
    "scheduler_step = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "scheduler_exp = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "print(\"학습률 스케줄러 준비 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 연습 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 1: 다중 클래스 분류 모델 구현\n",
    "# make_classification을 사용하여 3개 클래스 분류 문제를 해결하세요\n",
    "def create_multiclass_model(input_size, num_classes):\n",
    "    # 힌트: 마지막 층은 nn.Linear(?, num_classes)\n",
    "    # 손실 함수는 nn.CrossEntropyLoss() 사용\n",
    "    pass\n",
    "\n",
    "# 문제 2: 커스텀 활성화 함수 구현\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Swish(x) = x * sigmoid(x)\n",
    "        pass\n",
    "\n",
    "# 문제 3: 앙상블 모델 구현\n",
    "# 여러 모델의 예측을 평균내는 앙상블 클래스를 만드세요\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 모든 모델의 출력을 평균\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "이번 튜토리얼에서 배운 내용:\n",
    "1. PyTorch 텐서와 자동 미분\n",
    "2. nn.Module을 사용한 신경망 구축\n",
    "3. 손실 함수와 옵티마이저\n",
    "4. GPU 활용\n",
    "5. Dataset과 DataLoader로 효율적인 데이터 처리\n",
    "6. 모델 저장과 불러오기\n",
    "7. 다양한 최적화 기법과 팁\n",
    "\n",
    "### PyTorch의 장점:\n",
    "- **동적 계산 그래프**: 디버깅이 쉽고 직관적\n",
    "- **Pythonic**: Python다운 코드 작성 가능\n",
    "- **강력한 생태계**: torchvision, torchaudio 등 다양한 도구\n",
    "- **연구와 프로덕션**: 연구부터 배포까지 모두 가능\n",
    "\n",
    "다음 단계에서는 CNN을 사용하여 이미지 분류를 해보겠습니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}