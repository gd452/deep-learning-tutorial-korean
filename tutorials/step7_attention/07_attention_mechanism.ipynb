{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Attention Mechanism - LLM의 핵심 원리\n",
    "\n",
    "Attention은 \"Attention is All You Need\" 논문에서 소개된 혁신적인 메커니즘으로, 현대 LLM의 핵심입니다.\n",
    "\n",
    "## 학습 목표\n",
    "1. Attention의 직관적 이해\n",
    "2. Query, Key, Value 개념 마스터\n",
    "3. Scaled Dot-Product Attention 구현\n",
    "4. Multi-Head Attention 이해\n",
    "5. Positional Encoding의 필요성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "import math\n",
    "\n",
    "# 시각화 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# 재현성을 위한 시드 설정\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Attention의 직관적 이해\n",
    "\n",
    "Attention은 \"어디에 집중할 것인가\"를 학습하는 메커니즘입니다.\n",
    "\n",
    "### 예시: 문장 번역\n",
    "\"나는 사과를 좋아한다\" → \"I love apples\"\n",
    "\n",
    "- \"나는\" → \"I\"에 집중\n",
    "- \"좋아한다\" → \"love\"에 집중\n",
    "- \"사과를\" → \"apples\"에 집중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 Attention 예제\n",
    "def simple_attention_example():\n",
    "    # 예시 문장\n",
    "    source = [\"나는\", \"사과를\", \"좋아한다\"]\n",
    "    target = [\"I\", \"love\", \"apples\"]\n",
    "    \n",
    "    # 수동으로 만든 attention 점수 (실제로는 학습됨)\n",
    "    attention_scores = np.array([\n",
    "        [0.9, 0.05, 0.05],  # I → 나는(0.9), 사과를(0.05), 좋아한다(0.05)\n",
    "        [0.1, 0.1, 0.8],    # love → 나는(0.1), 사과를(0.1), 좋아한다(0.8)\n",
    "        [0.05, 0.9, 0.05]   # apples → 나는(0.05), 사과를(0.9), 좋아한다(0.05)\n",
    "    ])\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(attention_scores, \n",
    "                xticklabels=source, \n",
    "                yticklabels=target, \n",
    "                cmap='Blues', \n",
    "                annot=True, \n",
    "                fmt='.2f',\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    plt.title('Attention Weights: 번역 예시')\n",
    "    plt.xlabel('Source (한국어)')\n",
    "    plt.ylabel('Target (영어)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "simple_attention_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query, Key, Value의 개념\n",
    "\n",
    "Attention의 핵심은 Query(Q), Key(K), Value(V)입니다.\n",
    "\n",
    "### 직관적 비유: 도서관에서 책 찾기\n",
    "- **Query**: \"Python 프로그래밍에 관한 책을 찾고 싶어\" (내가 찾는 것)\n",
    "- **Key**: 각 책의 제목, 카테고리 (검색 가능한 정보)\n",
    "- **Value**: 실제 책의 내용 (가져올 정보)\n",
    "\n",
    "Query와 Key의 유사도가 높을수록, 해당 Value에 더 많이 집중합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query, Key, Value 예제\n",
    "def qkv_example():\n",
    "    # 간단한 예제: 3개의 단어, 4차원 임베딩\n",
    "    seq_len = 3\n",
    "    d_model = 4\n",
    "    \n",
    "    # 입력 시퀀스 (예: \"나는 학생이다\"의 임베딩)\n",
    "    x = torch.randn(seq_len, d_model)\n",
    "    \n",
    "    # Q, K, V 변환 행렬\n",
    "    W_q = torch.randn(d_model, d_model)\n",
    "    W_k = torch.randn(d_model, d_model)\n",
    "    W_v = torch.randn(d_model, d_model)\n",
    "    \n",
    "    # Q, K, V 계산\n",
    "    Q = x @ W_q  # Query\n",
    "    K = x @ W_k  # Key\n",
    "    V = x @ W_v  # Value\n",
    "    \n",
    "    print(\"입력 형태:\", x.shape)\n",
    "    print(\"Q 형태:\", Q.shape)\n",
    "    print(\"K 형태:\", K.shape)\n",
    "    print(\"V 형태:\", V.shape)\n",
    "    \n",
    "    # 시각화\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 3))\n",
    "    \n",
    "    matrices = [x, Q, K, V]\n",
    "    titles = ['Input (X)', 'Query (Q)', 'Key (K)', 'Value (V)']\n",
    "    \n",
    "    for i, (matrix, title) in enumerate(zip(matrices, titles)):\n",
    "        im = axes[i].imshow(matrix.detach().numpy(), cmap='coolwarm', aspect='auto')\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_xlabel('Dimension')\n",
    "        axes[i].set_ylabel('Position')\n",
    "        plt.colorbar(im, ax=axes[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return Q, K, V\n",
    "\n",
    "Q, K, V = qkv_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaled Dot-Product Attention\n",
    "\n",
    "Attention의 핵심 공식:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "여기서:\n",
    "- $QK^T$: Query와 Key의 유사도 계산\n",
    "- $\\sqrt{d_k}$: 스케일링 (값이 너무 커지는 것을 방지)\n",
    "- softmax: 확률 분포로 변환\n",
    "- V: Value에 가중치 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention 구현\n",
    "    \n",
    "    Args:\n",
    "        Q: Query (seq_len, d_k)\n",
    "        K: Key (seq_len, d_k)\n",
    "        V: Value (seq_len, d_v)\n",
    "        mask: 마스킹 (선택사항)\n",
    "    \n",
    "    Returns:\n",
    "        attention_output: 어텐션 결과\n",
    "        attention_weights: 어텐션 가중치\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # 1. Q와 K의 유사도 계산\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # 2. 마스킹 (선택사항)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # 3. Softmax로 확률 분포 생성\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 4. Value에 가중치 적용\n",
    "    attention_output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return attention_output, attention_weights\n",
    "\n",
    "# 예제 실행\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(\"Attention 출력 형태:\", output.shape)\n",
    "print(\"Attention 가중치 형태:\", weights.shape)\n",
    "\n",
    "# Attention 가중치 시각화\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(weights.detach().numpy(), \n",
    "            annot=True, \n",
    "            fmt='.3f', \n",
    "            cmap='Blues',\n",
    "            xticklabels=['Pos 0', 'Pos 1', 'Pos 2'],\n",
    "            yticklabels=['Pos 0', 'Pos 1', 'Pos 2'])\n",
    "plt.title('Attention Weights Matrix')\n",
    "plt.xlabel('Key Positions')\n",
    "plt.ylabel('Query Positions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 단계별 Attention 계산 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_steps():\n",
    "    # 더 간단한 예제로 단계별 설명\n",
    "    seq_len = 4\n",
    "    d_k = 3\n",
    "    \n",
    "    # 예시 데이터\n",
    "    Q = torch.tensor([[1.0, 0.0, 1.0],\n",
    "                      [0.0, 1.0, 0.0],\n",
    "                      [1.0, 1.0, 0.0],\n",
    "                      [0.0, 0.0, 1.0]])\n",
    "    \n",
    "    K = torch.tensor([[1.0, 0.0, 0.0],\n",
    "                      [0.0, 1.0, 0.0],\n",
    "                      [0.0, 0.0, 1.0],\n",
    "                      [1.0, 1.0, 0.0]])\n",
    "    \n",
    "    V = torch.tensor([[1.0, 0.0],\n",
    "                      [0.0, 1.0],\n",
    "                      [1.0, 1.0],\n",
    "                      [0.5, 0.5]])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    # Step 1: Q와 K 표시\n",
    "    axes[0, 0].imshow(Q, cmap='Blues', aspect='auto')\n",
    "    axes[0, 0].set_title('Query (Q)')\n",
    "    axes[0, 0].set_ylabel('Positions')\n",
    "    axes[0, 0].set_xlabel('Dimensions')\n",
    "    \n",
    "    axes[0, 1].imshow(K, cmap='Oranges', aspect='auto')\n",
    "    axes[0, 1].set_title('Key (K)')\n",
    "    axes[0, 1].set_ylabel('Positions')\n",
    "    axes[0, 1].set_xlabel('Dimensions')\n",
    "    \n",
    "    # Step 2: QK^T 계산\n",
    "    scores = torch.matmul(Q, K.transpose(0, 1))\n",
    "    axes[0, 2].imshow(scores, cmap='Greens', aspect='auto')\n",
    "    axes[0, 2].set_title(r'$QK^T$ (Similarity Scores)')\n",
    "    axes[0, 2].set_ylabel('Query Positions')\n",
    "    axes[0, 2].set_xlabel('Key Positions')\n",
    "    \n",
    "    # Step 3: Scaling\n",
    "    scaled_scores = scores / math.sqrt(d_k)\n",
    "    axes[1, 0].imshow(scaled_scores, cmap='Greens', aspect='auto')\n",
    "    axes[1, 0].set_title(r'$\\frac{QK^T}{\\sqrt{d_k}}$ (Scaled Scores)')\n",
    "    axes[1, 0].set_ylabel('Query Positions')\n",
    "    axes[1, 0].set_xlabel('Key Positions')\n",
    "    \n",
    "    # Step 4: Softmax\n",
    "    attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "    im = axes[1, 1].imshow(attention_weights, cmap='Reds', aspect='auto')\n",
    "    axes[1, 1].set_title('Softmax(Scaled Scores)')\n",
    "    axes[1, 1].set_ylabel('Query Positions')\n",
    "    axes[1, 1].set_xlabel('Key Positions')\n",
    "    \n",
    "    # 값 표시\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            axes[1, 1].text(j, i, f'{attention_weights[i, j]:.2f}', \n",
    "                           ha='center', va='center')\n",
    "    \n",
    "    # Step 5: Attention Output\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    axes[1, 2].imshow(output, cmap='Purples', aspect='auto')\n",
    "    axes[1, 2].set_title('Attention Output')\n",
    "    axes[1, 2].set_ylabel('Positions')\n",
    "    axes[1, 2].set_xlabel('Output Dimensions')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-Attention\n",
    "\n",
    "Self-Attention은 같은 시퀀스 내에서 각 위치가 다른 모든 위치를 참조하는 메커니즘입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Q, K, V를 위한 선형 변환\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Self-Attention 예제\n",
    "def self_attention_example():\n",
    "    # 예시 문장: \"나는 학교에 간다\"\n",
    "    words = [\"나는\", \"학교에\", \"간다\"]\n",
    "    seq_len = len(words)\n",
    "    d_model = 8\n",
    "    \n",
    "    # 임의의 단어 임베딩\n",
    "    x = torch.randn(1, seq_len, d_model)\n",
    "    \n",
    "    # Self-Attention 적용\n",
    "    self_attn = SelfAttention(d_model)\n",
    "    output, weights = self_attn(x)\n",
    "    \n",
    "    # 가중치 시각화\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(weights[0].detach().numpy(), \n",
    "                annot=True, \n",
    "                fmt='.3f',\n",
    "                xticklabels=words,\n",
    "                yticklabels=words,\n",
    "                cmap='YlOrRd')\n",
    "    plt.title('Self-Attention Weights')\n",
    "    plt.xlabel('참조하는 단어')\n",
    "    plt.ylabel('현재 단어')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 해석\n",
    "    print(\"\\nSelf-Attention 해석:\")\n",
    "    for i, word in enumerate(words):\n",
    "        print(f\"\\n'{word}'가 주목하는 단어:\")\n",
    "        for j, ref_word in enumerate(words):\n",
    "            print(f\"  - '{ref_word}': {weights[0, i, j]:.3f}\")\n",
    "\n",
    "self_attention_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Head Attention\n",
    "\n",
    "Multi-Head Attention은 여러 개의 attention을 병렬로 수행하여 다양한 관점에서 정보를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # 각 헤드를 위한 선형 변환\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # 1. Q, K, V 계산\n",
    "        Q = self.W_q(x)  # (batch_size, seq_len, d_model)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # 2. 헤드별로 분리\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # 현재 형태: (batch_size, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 3. Scaled Dot-Product Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # 4. 헤드 합치기\n",
    "        context = context.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        # 5. 최종 선형 변환\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Multi-Head Attention 시각화\n",
    "def visualize_multihead_attention():\n",
    "    seq_len = 4\n",
    "    d_model = 8\n",
    "    num_heads = 2\n",
    "    \n",
    "    # 입력 생성\n",
    "    x = torch.randn(1, seq_len, d_model)\n",
    "    \n",
    "    # Multi-Head Attention\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    output, weights = mha(x)\n",
    "    \n",
    "    # 각 헤드의 attention 가중치 시각화\n",
    "    fig, axes = plt.subplots(1, num_heads, figsize=(12, 5))\n",
    "    \n",
    "    for head in range(num_heads):\n",
    "        ax = axes[head]\n",
    "        sns.heatmap(weights[0, head].detach().numpy(),\n",
    "                   annot=True,\n",
    "                   fmt='.2f',\n",
    "                   cmap='Blues',\n",
    "                   ax=ax)\n",
    "        ax.set_title(f'Head {head + 1}')\n",
    "        ax.set_xlabel('Key Positions')\n",
    "        ax.set_ylabel('Query Positions')\n",
    "    \n",
    "    plt.suptitle('Multi-Head Attention Weights', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"입력 형태: {x.shape}\")\n",
    "    print(f\"출력 형태: {output.shape}\")\n",
    "    print(f\"\\n각 헤드는 서로 다른 패턴에 주목합니다!\")\n",
    "\n",
    "visualize_multihead_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Positional Encoding\n",
    "\n",
    "Attention은 순서 정보가 없으므로, 위치 정보를 추가해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # 위치 인코딩 계산\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        \n",
    "        # 주파수 계산\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # 사인과 코사인 적용\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n",
    "\n",
    "# Positional Encoding 시각화\n",
    "def visualize_positional_encoding():\n",
    "    d_model = 128\n",
    "    max_seq_len = 100\n",
    "    \n",
    "    pe = PositionalEncoding(d_model, max_seq_len)\n",
    "    \n",
    "    # 위치 인코딩 값 가져오기\n",
    "    encoding = pe.pe[0, :50, :].numpy()\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(encoding.T, cmap='RdBu', aspect='auto')\n",
    "    plt.colorbar(label='Value')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Dimension')\n",
    "    plt.title('Positional Encoding Pattern')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 특정 차원의 패턴 보기\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    dimensions = [0, 1, 10, 11]\n",
    "    for i, dim in enumerate(dimensions):\n",
    "        axes[i].plot(encoding[:, dim])\n",
    "        axes[i].set_title(f'Dimension {dim} ({\"sin\" if dim % 2 == 0 else \"cos\"})')\n",
    "        axes[i].set_xlabel('Position')\n",
    "        axes[i].set_ylabel('Value')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_positional_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Masked Attention (Causal Attention)\n",
    "\n",
    "언어 모델에서는 미래 정보를 보지 못하도록 마스킹을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Causal mask 생성 (미래 정보 차단)\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    return mask == 0\n",
    "\n",
    "def masked_attention_example():\n",
    "    seq_len = 5\n",
    "    d_model = 8\n",
    "    \n",
    "    # 예시 문장: \"나는 오늘 학교에 갔다\"\n",
    "    words = [\"나는\", \"오늘\", \"학교에\", \"갔다\", \"<END>\"]\n",
    "    \n",
    "    # 입력과 마스크 생성\n",
    "    x = torch.randn(1, seq_len, d_model)\n",
    "    mask = create_causal_mask(seq_len)\n",
    "    \n",
    "    # Attention 계산\n",
    "    self_attn = SelfAttention(d_model)\n",
    "    Q = self_attn.W_q(x)\n",
    "    K = self_attn.W_k(x)\n",
    "    V = self_attn.W_v(x)\n",
    "    \n",
    "    # Masked attention\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model)\n",
    "    scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 시각화\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # 1. Causal Mask\n",
    "    axes[0].imshow(mask.float(), cmap='gray', aspect='auto')\n",
    "    axes[0].set_title('Causal Mask\\n(흰색=볼 수 있음, 검은색=볼 수 없음)')\n",
    "    axes[0].set_xlabel('Key Position')\n",
    "    axes[0].set_ylabel('Query Position')\n",
    "    \n",
    "    # 라벨 추가\n",
    "    for i in range(seq_len):\n",
    "        axes[0].set_xticks(range(seq_len))\n",
    "        axes[0].set_xticklabels(words, rotation=45)\n",
    "        axes[0].set_yticks(range(seq_len))\n",
    "        axes[0].set_yticklabels(words)\n",
    "    \n",
    "    # 2. Raw Scores\n",
    "    raw_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model)\n",
    "    im = axes[1].imshow(raw_scores[0].detach().numpy(), cmap='Blues', aspect='auto')\n",
    "    axes[1].set_title('Raw Attention Scores')\n",
    "    axes[1].set_xticks(range(seq_len))\n",
    "    axes[1].set_xticklabels(words, rotation=45)\n",
    "    axes[1].set_yticks(range(seq_len))\n",
    "    axes[1].set_yticklabels(words)\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "    \n",
    "    # 3. Masked Attention Weights\n",
    "    im = axes[2].imshow(attention_weights[0].detach().numpy(), cmap='Reds', aspect='auto')\n",
    "    axes[2].set_title('Masked Attention Weights')\n",
    "    axes[2].set_xticks(range(seq_len))\n",
    "    axes[2].set_xticklabels(words, rotation=45)\n",
    "    axes[2].set_yticks(range(seq_len))\n",
    "    axes[2].set_yticklabels(words)\n",
    "    plt.colorbar(im, ax=axes[2])\n",
    "    \n",
    "    # 값 표시\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if mask[i, j]:\n",
    "                axes[2].text(j, i, f'{attention_weights[0, i, j]:.2f}', \n",
    "                           ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nCausal Attention 설명:\")\n",
    "    print(\"- '나는'은 자기 자신만 볼 수 있음\")\n",
    "    print(\"- '오늘'은 '나는'과 자기 자신을 볼 수 있음\")\n",
    "    print(\"- '학교에'는 '나는', '오늘', 자기 자신을 볼 수 있음\")\n",
    "    print(\"- 이런 식으로 미래 단어는 볼 수 없음!\")\n",
    "\n",
    "masked_attention_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 실제 문장에서 Attention 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_sentence_attention():\n",
    "    # 예시 문장과 간단한 임베딩\n",
    "    sentence = \"The cat sat on the mat\"\n",
    "    words = sentence.split()\n",
    "    vocab = {word: i for i, word in enumerate(set(words))}\n",
    "    \n",
    "    # 원-핫 인코딩 후 임베딩\n",
    "    d_model = 8\n",
    "    embedding = nn.Embedding(len(vocab), d_model)\n",
    "    \n",
    "    # 단어를 인덱스로 변환\n",
    "    indices = torch.tensor([vocab[word] for word in words])\n",
    "    x = embedding(indices).unsqueeze(0)  # (1, seq_len, d_model)\n",
    "    \n",
    "    # Self-Attention 적용\n",
    "    self_attn = SelfAttention(d_model)\n",
    "    output, weights = self_attn(x)\n",
    "    \n",
    "    # Attention 패턴 시각화\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(weights[0].detach().numpy(),\n",
    "                xticklabels=words,\n",
    "                yticklabels=words,\n",
    "                annot=True,\n",
    "                fmt='.3f',\n",
    "                cmap='YlOrRd',\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    plt.title('Self-Attention on: \"The cat sat on the mat\"')\n",
    "    plt.xlabel('Attending to (참조하는 단어)')\n",
    "    plt.ylabel('From (현재 단어)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 특정 단어의 attention 분석\n",
    "    word_idx = 1  # \"cat\"\n",
    "    print(f\"\\n'{words[word_idx]}'이(가) 주목하는 단어들:\")\n",
    "    attention_scores = weights[0, word_idx].detach().numpy()\n",
    "    for i, (word, score) in enumerate(zip(words, attention_scores)):\n",
    "        print(f\"  {word}: {score:.3f} {'***' if score > 0.2 else ''}\")\n",
    "\n",
    "real_sentence_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Attention의 장점과 특징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_advantages():\n",
    "    # Attention의 장점 시각화\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. Long-range dependencies\n",
    "    ax = axes[0, 0]\n",
    "    sentence = \"The student who studied hard passed the exam\"\n",
    "    words = sentence.split()\n",
    "    positions = list(range(len(words)))\n",
    "    \n",
    "    # RNN vs Attention 경로\n",
    "    ax.scatter(positions, [1]*len(words), s=100)\n",
    "    for i, word in enumerate(words):\n",
    "        ax.text(i, 1, word, ha='center', va='bottom', rotation=45)\n",
    "    \n",
    "    # RNN 경로 (순차적)\n",
    "    for i in range(len(words)-1):\n",
    "        ax.arrow(i, 0.5, 0.8, 0, head_width=0.1, head_length=0.1, \n",
    "                fc='blue', alpha=0.5)\n",
    "    \n",
    "    # Attention 경로 (직접 연결)\n",
    "    ax.arrow(1, 1.5, 5, 0, head_width=0.1, head_length=0.2, \n",
    "            fc='red', alpha=0.7, linestyle='--')\n",
    "    \n",
    "    ax.set_ylim(0, 2)\n",
    "    ax.set_title('Long-range Dependencies\\nRNN(파란색) vs Attention(빨간색)')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 2. Parallelization\n",
    "    ax = axes[0, 1]\n",
    "    ax.text(0.5, 0.8, 'RNN: Sequential Processing', ha='center', fontsize=12)\n",
    "    ax.text(0.5, 0.7, 't=1 → t=2 → t=3 → t=4', ha='center', fontsize=10)\n",
    "    ax.text(0.5, 0.5, 'Attention: Parallel Processing', ha='center', fontsize=12)\n",
    "    ax.text(0.5, 0.4, 't=1, t=2, t=3, t=4 (동시에!)', ha='center', fontsize=10)\n",
    "    ax.text(0.5, 0.2, '⚡ 훨씬 빠른 학습!', ha='center', fontsize=14, color='green')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('병렬 처리의 장점')\n",
    "    \n",
    "    # 3. Interpretability\n",
    "    ax = axes[1, 0]\n",
    "    attention_matrix = np.random.rand(5, 5)\n",
    "    attention_matrix = attention_matrix / attention_matrix.sum(axis=1, keepdims=True)\n",
    "    im = ax.imshow(attention_matrix, cmap='Blues')\n",
    "    ax.set_title('Attention의 해석 가능성\\n(어디에 주목하는지 볼 수 있음)')\n",
    "    ax.set_xlabel('Input positions')\n",
    "    ax.set_ylabel('Output positions')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # 4. 계산 복잡도 비교\n",
    "    ax = axes[1, 1]\n",
    "    seq_lengths = np.array([10, 50, 100, 200, 500])\n",
    "    rnn_complexity = seq_lengths  # O(n)\n",
    "    attention_complexity = seq_lengths ** 2  # O(n²)\n",
    "    \n",
    "    ax.plot(seq_lengths, rnn_complexity, 'b-', label='RNN: O(n)', linewidth=2)\n",
    "    ax.plot(seq_lengths, attention_complexity, 'r-', label='Attention: O(n²)', linewidth=2)\n",
    "    ax.set_xlabel('Sequence Length')\n",
    "    ax.set_ylabel('Computational Complexity')\n",
    "    ax.set_title('계산 복잡도 비교')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nAttention의 주요 장점:\")\n",
    "    print(\"1. 장거리 의존성: 멀리 떨어진 단어도 직접 참조 가능\")\n",
    "    print(\"2. 병렬 처리: 모든 위치를 동시에 계산 → 빠른 학습\")\n",
    "    print(\"3. 해석 가능성: Attention 가중치로 모델의 동작 이해 가능\")\n",
    "    print(\"\\n단점:\")\n",
    "    print(\"- 메모리 사용량: O(n²) → 긴 시퀀스에서 문제\")\n",
    "    print(\"- 위치 정보 부재: Positional Encoding 필요\")\n",
    "\n",
    "attention_advantages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 연습 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 1: Cross-Attention 구현\n",
    "# Encoder의 출력을 참조하는 Decoder attention\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        # 힌트: Q는 decoder에서, K와 V는 encoder에서 옵니다\n",
    "        # TODO: 구현하기\n",
    "        pass\n",
    "    \n",
    "    def forward(self, decoder_input, encoder_output):\n",
    "        # TODO: 구현하기\n",
    "        pass\n",
    "\n",
    "# 문제 2: Relative Position Encoding\n",
    "# 절대 위치가 아닌 상대 위치 인코딩 구현\n",
    "def relative_position_encoding(seq_len, d_model):\n",
    "    # 힌트: 각 위치 쌍 (i, j)에 대해 i-j를 인코딩\n",
    "    # TODO: 구현하기\n",
    "    pass\n",
    "\n",
    "# 문제 3: Sparse Attention\n",
    "# 모든 위치가 아닌 일부만 참조하는 attention\n",
    "def sparse_attention_pattern(seq_len, window_size):\n",
    "    # 힌트: 각 위치는 window_size 범위 내의 위치만 참조\n",
    "    # TODO: 구현하기\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "이번 튜토리얼에서 배운 내용:\n",
    "1. **Attention의 직관적 이해**: 어디에 주목할지 학습하는 메커니즘\n",
    "2. **Query, Key, Value**: 검색 시스템과 유사한 개념\n",
    "3. **Scaled Dot-Product Attention**: 핵심 계산 과정\n",
    "4. **Multi-Head Attention**: 다양한 관점에서 정보 추출\n",
    "5. **Positional Encoding**: 순서 정보 추가\n",
    "6. **Masked Attention**: 언어 모델을 위한 미래 정보 차단\n",
    "\n",
    "### 핵심 공식 다시 보기:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### Attention이 혁신적인 이유:\n",
    "- **병렬 처리**: RNN과 달리 모든 위치를 동시에 계산\n",
    "- **장거리 의존성**: 멀리 떨어진 정보도 직접 참조\n",
    "- **해석 가능성**: Attention 가중치로 모델 동작 이해\n",
    "\n",
    "다음 단계에서는 이 Attention을 사용하여 완전한 Transformer를 구축해보겠습니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}