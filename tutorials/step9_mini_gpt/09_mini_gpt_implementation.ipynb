{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Mini GPT 구현 - 나만의 언어 모델 만들기\n",
    "\n",
    "이제 지금까지 배운 모든 것을 종합하여 작은 GPT 모델을 처음부터 구현해봅시다!\n",
    "\n",
    "## 학습 목표\n",
    "1. GPT 아키텍처 이해 (Decoder-only Transformer)\n",
    "2. 텍스트 토큰화와 데이터 준비\n",
    "3. 모델 학습 파이프라인 구축\n",
    "4. 텍스트 생성 전략 구현\n",
    "5. 모델 평가와 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "\n",
    "# 시각화 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# 재현성\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 토큰화 (Tokenization)\n",
    "\n",
    "텍스트를 모델이 이해할 수 있는 토큰으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"간단한 문자 수준 토크나이저\"\"\"\n",
    "    def __init__(self):\n",
    "        self.char_to_idx = {}\n",
    "        self.idx_to_char = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        \"\"\"텍스트에서 어휘 구축\"\"\"\n",
    "        # 모든 고유 문자 수집\n",
    "        chars = set()\n",
    "        for text in texts:\n",
    "            chars.update(text)\n",
    "        \n",
    "        # 특수 토큰 추가\n",
    "        chars = ['<PAD>', '<SOS>', '<EOS>', '<UNK>'] + sorted(list(chars))\n",
    "        \n",
    "        # 인덱스 매핑 생성\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "        self.idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "        self.vocab_size = len(chars)\n",
    "        \n",
    "        print(f\"어휘 크기: {self.vocab_size}\")\n",
    "        print(f\"어휘 예시: {chars[:20]}...\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"텍스트를 인덱스로 변환\"\"\"\n",
    "        return [self.char_to_idx.get(char, self.char_to_idx['<UNK>']) for char in text]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"인덱스를 텍스트로 변환\"\"\"\n",
    "        return ''.join([self.idx_to_char.get(idx, '<UNK>') for idx in indices])\n",
    "\n",
    "# 더 고급 토크나이저: 단어 수준\n",
    "class WordTokenizer:\n",
    "    \"\"\"단어 수준 토크나이저\"\"\"\n",
    "    def __init__(self, vocab_size=None):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "        self.max_vocab_size = vocab_size\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        \"\"\"텍스트에서 어휘 구축\"\"\"\n",
    "        # 단어 빈도 계산\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # 빈도순으로 정렬\n",
    "        if self.max_vocab_size:\n",
    "            most_common = word_counts.most_common(self.max_vocab_size - 4)\n",
    "        else:\n",
    "            most_common = word_counts.most_common()\n",
    "        \n",
    "        # 특수 토큰 추가\n",
    "        vocab = ['<PAD>', '<SOS>', '<EOS>', '<UNK>'] + [word for word, _ in most_common]\n",
    "        \n",
    "        # 인덱스 매핑\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "        self.vocab_size = len(vocab)\n",
    "        \n",
    "        print(f\"어휘 크기: {self.vocab_size}\")\n",
    "        print(f\"가장 빈번한 단어: {[word for word, _ in most_common[:10]]}\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"텍스트를 인덱스로 변환\"\"\"\n",
    "        words = text.lower().split()\n",
    "        return [self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in words]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"인덱스를 텍스트로 변환\"\"\"\n",
    "        words = [self.idx_to_word.get(idx, '<UNK>') for idx in indices]\n",
    "        return ' '.join(words)\n",
    "\n",
    "# 예시 텍스트\n",
    "sample_texts = [\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Neural networks are inspired by the human brain.\",\n",
    "    \"Transformers have revolutionized natural language processing.\"\n",
    "]\n",
    "\n",
    "# 토크나이저 테스트\n",
    "char_tokenizer = SimpleTokenizer()\n",
    "char_tokenizer.fit(sample_texts)\n",
    "\n",
    "# 인코딩/디코딩 예시\n",
    "test_text = \"Deep learning\"\n",
    "encoded = char_tokenizer.encode(test_text)\n",
    "decoded = char_tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\n원본 텍스트: '{test_text}'\")\n",
    "print(f\"인코딩: {encoded}\")\n",
    "print(f\"디코딩: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    \"\"\"GPT 학습을 위한 데이터셋\"\"\"\n",
    "    def __init__(self, texts, tokenizer, seq_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # 모든 텍스트를 하나로 합치고 토큰화\n",
    "        all_text = ' '.join(texts)\n",
    "        self.tokens = tokenizer.encode(all_text)\n",
    "        \n",
    "        print(f\"총 토큰 수: {len(self.tokens)}\")\n",
    "        print(f\"시퀀스 길이: {seq_length}\")\n",
    "        print(f\"가능한 시퀀스 수: {len(self.tokens) - seq_length}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 입력: [idx:idx+seq_length]\n",
    "        # 타겟: [idx+1:idx+seq_length+1]\n",
    "        input_seq = torch.tensor(self.tokens[idx:idx+self.seq_length], dtype=torch.long)\n",
    "        target_seq = torch.tensor(self.tokens[idx+1:idx+self.seq_length+1], dtype=torch.long)\n",
    "        \n",
    "        return input_seq, target_seq\n",
    "\n",
    "# 더 큰 텍스트 데이터 생성 (셰익스피어 스타일)\n",
    "shakespeare_style = \"\"\"\n",
    "To be or not to be that is the question.\n",
    "Whether tis nobler in the mind to suffer the slings and arrows of outrageous fortune.\n",
    "Or to take arms against a sea of troubles and by opposing end them.\n",
    "To die to sleep no more and by a sleep to say we end.\n",
    "The heartache and the thousand natural shocks that flesh is heir to.\n",
    "Tis a consummation devoutly to be wished to die to sleep.\n",
    "To sleep perchance to dream ay theres the rub.\n",
    "For in that sleep of death what dreams may come.\n",
    "\"\"\"\n",
    "\n",
    "# 데이터셋 생성\n",
    "texts = [shakespeare_style] * 10  # 더 많은 데이터를 위해 반복\n",
    "dataset = GPTDataset(texts, char_tokenizer, seq_length=64)\n",
    "\n",
    "# 샘플 확인\n",
    "input_seq, target_seq = dataset[0]\n",
    "print(f\"\\n입력 시퀀스: {input_seq.shape}\")\n",
    "print(f\"타겟 시퀀스: {target_seq.shape}\")\n",
    "print(f\"\\n입력 텍스트: '{char_tokenizer.decode(input_seq.tolist()[:20])}...'\")\n",
    "print(f\"타겟 텍스트: '{char_tokenizer.decode(target_seq.tolist()[:20])}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mini GPT 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Q, K, V 계산\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Attention 계산\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(self.gelu(self.fc1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual\n",
    "        attn_output, attn_weights = self.attention(self.ln1(x), mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        ff_output = self.feed_forward(self.ln2(x))\n",
    "        x = x + self.dropout(ff_output)\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, n_heads=8, n_layers=6, \n",
    "                 d_ff=1024, max_seq_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # 토큰 임베딩\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer 블록\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # 최종 레이어\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        # 위치 인덱스\n",
    "        positions = torch.arange(0, seq_len).expand(batch_size, seq_len).to(x.device)\n",
    "        \n",
    "        # 임베딩\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "        \n",
    "        # Causal mask 생성\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len).to(x.device)\n",
    "        \n",
    "        # Transformer 블록 통과\n",
    "        attention_weights = []\n",
    "        for block in self.blocks:\n",
    "            x, attn_w = block(x, mask)\n",
    "            attention_weights.append(attn_w)\n",
    "        \n",
    "        # 최종 출력\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.fc_out(x)\n",
    "        \n",
    "        # 손실 계산 (학습 시)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss, attention_weights\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=True):\n",
    "        \"\"\"텍스트 생성\"\"\"\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 컨텍스트 크기 제한\n",
    "            idx_cond = idx if idx.size(1) <= self.max_seq_len else idx[:, -self.max_seq_len:]\n",
    "            \n",
    "            # 예측\n",
    "            logits, _, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # 샘플링\n",
    "            if do_sample:\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # 다음 토큰 추가\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "# 모델 생성\n",
    "model = MiniGPT(\n",
    "    vocab_size=char_tokenizer.vocab_size,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    n_layers=4,\n",
    "    d_ff=512,\n",
    "    max_seq_len=256,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# 모델 정보\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Mini GPT 모델 생성 완료!\")\n",
    "print(f\"총 파라미터 수: {total_params:,}\")\n",
    "print(f\"모델 크기: {total_params * 4 / 1024 / 1024:.2f} MB (float32 기준)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs=10, batch_size=32, learning_rate=3e-4):\n",
    "    \"\"\"모델 학습\"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 학습률 스케줄러 (선택사항)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, loss, _ = model(inputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # 샘플 생성 (매 에폭마다)\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            generate_sample(model, char_tokenizer, \"To be or \")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def generate_sample(model, tokenizer, prompt, max_length=100, temperature=1.0):\n",
    "    \"\"\"샘플 텍스트 생성\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 프롬프트 인코딩\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    x = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 생성\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(x, max_length, temperature)\n",
    "    \n",
    "    # 디코딩\n",
    "    generated_text = tokenizer.decode(generated[0].tolist())\n",
    "    print(f\"\\n생성된 텍스트: '{generated_text}'\\n\")\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "# 학습 실행\n",
    "print(\"학습 시작...\")\n",
    "losses = train_model(model, dataset, epochs=20, batch_size=16, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 학습 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 다양한 온도로 텍스트 생성\n",
    "def generate_with_different_temperatures(model, tokenizer, prompt):\n",
    "    temperatures = [0.5, 0.8, 1.0, 1.2, 1.5]\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n--- Temperature: {temp} ---\")\n",
    "        model.eval()\n",
    "        \n",
    "        tokens = tokenizer.encode(prompt)\n",
    "        x = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(x, 100, temperature=temp)\n",
    "        \n",
    "        generated_text = tokenizer.decode(generated[0].tolist())\n",
    "        print(generated_text)\n",
    "\n",
    "print(\"다양한 온도로 텍스트 생성:\")\n",
    "generate_with_different_temperatures(model, char_tokenizer, \"To be or \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Attention 패턴 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_patterns(model, tokenizer, text):\n",
    "    \"\"\"Attention 패턴 시각화\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 텍스트 인코딩\n",
    "    tokens = tokenizer.encode(text)\n",
    "    x = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        logits, _, attention_weights = model(x)\n",
    "    \n",
    "    # 첫 번째 레이어, 첫 번째 헤드의 attention 시각화\n",
    "    attn = attention_weights[0][0, 0].cpu().numpy()  # (seq_len, seq_len)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attn[:len(text), :len(text)], \n",
    "                xticklabels=list(text),\n",
    "                yticklabels=list(text),\n",
    "                cmap='Blues',\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    plt.title('Attention Pattern (Layer 1, Head 1)')\n",
    "    plt.xlabel('Keys')\n",
    "    plt.ylabel('Queries')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 여러 레이어의 평균 attention\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(4, len(attention_weights))):\n",
    "        avg_attn = attention_weights[i][0].mean(dim=0).cpu().numpy()\n",
    "        \n",
    "        im = axes[i].imshow(avg_attn[:len(text), :len(text)], \n",
    "                           cmap='Blues', aspect='auto')\n",
    "        axes[i].set_title(f'Layer {i+1} (Average across heads)')\n",
    "        axes[i].set_xlabel('Position')\n",
    "        axes[i].set_ylabel('Position')\n",
    "        plt.colorbar(im, ax=axes[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Attention 패턴 시각화\n",
    "test_text = \"To be or not to be\"\n",
    "print(f\"텍스트: '{test_text}'\")\n",
    "visualize_attention_patterns(model, char_tokenizer, test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 생성 전략 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedGenerator:\n",
    "    \"\"\"고급 텍스트 생성 전략\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def greedy_search(model, tokenizer, prompt, max_length=100):\n",
    "        \"\"\"Greedy 디코딩\"\"\"\n",
    "        model.eval()\n",
    "        tokens = tokenizer.encode(prompt)\n",
    "        x = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(x, max_length, temperature=1.0, do_sample=False)\n",
    "        \n",
    "        return tokenizer.decode(generated[0].tolist())\n",
    "    \n",
    "    @staticmethod\n",
    "    def top_k_sampling(model, tokenizer, prompt, max_length=100, k=10, temperature=1.0):\n",
    "        \"\"\"Top-k 샘플링\"\"\"\n",
    "        model.eval()\n",
    "        tokens = tokenizer.encode(prompt)\n",
    "        x = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            with torch.no_grad():\n",
    "                logits, _, _ = model(x)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Top-k 필터링\n",
    "            top_k_values, top_k_indices = torch.topk(logits, k, dim=-1)\n",
    "            probs = F.softmax(top_k_values, dim=-1)\n",
    "            \n",
    "            # 샘플링\n",
    "            idx = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = top_k_indices.gather(-1, idx)\n",
    "            \n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "        \n",
    "        return tokenizer.decode(x[0].tolist())\n",
    "    \n",
    "    @staticmethod\n",
    "    def top_p_sampling(model, tokenizer, prompt, max_length=100, p=0.9, temperature=1.0):\n",
    "        \"\"\"Top-p (nucleus) 샘플링\"\"\"\n",
    "        model.eval()\n",
    "        tokens = tokenizer.encode(prompt)\n",
    "        x = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            with torch.no_grad():\n",
    "                logits, _, _ = model(x)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # 확률 계산 및 정렬\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "            \n",
    "            # 누적 확률 계산\n",
    "            cumsum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "            \n",
    "            # Top-p 마스크\n",
    "            mask = cumsum_probs > p\n",
    "            mask[..., 1:] = mask[..., :-1].clone()\n",
    "            mask[..., 0] = False\n",
    "            \n",
    "            # 필터링된 확률에서 샘플링\n",
    "            sorted_probs[mask] = 0\n",
    "            sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "            \n",
    "            idx = torch.multinomial(sorted_probs, num_samples=1)\n",
    "            next_token = sorted_indices.gather(-1, idx)\n",
    "            \n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "        \n",
    "        return tokenizer.decode(x[0].tolist())\n",
    "\n",
    "# 다양한 생성 전략 비교\n",
    "prompt = \"To be or \"\n",
    "generator = AdvancedGenerator()\n",
    "\n",
    "print(\"=== 다양한 생성 전략 비교 ===\")\n",
    "print(f\"프롬프트: '{prompt}'\\n\")\n",
    "\n",
    "print(\"1. Greedy Search:\")\n",
    "print(generator.greedy_search(model, char_tokenizer, prompt, 50))\n",
    "\n",
    "print(\"\\n2. Top-k Sampling (k=5):\")\n",
    "print(generator.top_k_sampling(model, char_tokenizer, prompt, 50, k=5))\n",
    "\n",
    "print(\"\\n3. Top-p Sampling (p=0.9):\")\n",
    "print(generator.top_p_sampling(model, char_tokenizer, prompt, 50, p=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 모델 저장과 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "def save_model(model, tokenizer, path='mini_gpt_model'):\n",
    "    \"\"\"모델과 토크나이저 저장\"\"\"\n",
    "    # 디렉토리 생성\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    # 모델 저장\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'vocab_size': model.token_embedding.num_embeddings,\n",
    "            'd_model': model.d_model,\n",
    "            'n_heads': model.blocks[0].attention.n_heads,\n",
    "            'n_layers': len(model.blocks),\n",
    "            'd_ff': model.blocks[0].feed_forward.fc1.out_features,\n",
    "            'max_seq_len': model.max_seq_len,\n",
    "        }\n",
    "    }, os.path.join(path, 'model.pt'))\n",
    "    \n",
    "    # 토크나이저 저장\n",
    "    tokenizer_data = {\n",
    "        'char_to_idx': tokenizer.char_to_idx,\n",
    "        'idx_to_char': tokenizer.idx_to_char,\n",
    "        'vocab_size': tokenizer.vocab_size\n",
    "    }\n",
    "    with open(os.path.join(path, 'tokenizer.json'), 'w') as f:\n",
    "        json.dump(tokenizer_data, f)\n",
    "    \n",
    "    print(f\"모델이 '{path}' 디렉토리에 저장되었습니다.\")\n",
    "\n",
    "# 모델 불러오기\n",
    "def load_model(path='mini_gpt_model'):\n",
    "    \"\"\"저장된 모델과 토크나이저 불러오기\"\"\"\n",
    "    # 체크포인트 불러오기\n",
    "    checkpoint = torch.load(os.path.join(path, 'model.pt'), map_location=device)\n",
    "    \n",
    "    # 모델 재생성\n",
    "    config = checkpoint['model_config']\n",
    "    model = MiniGPT(\n",
    "        vocab_size=config['vocab_size'],\n",
    "        d_model=config['d_model'],\n",
    "        n_heads=config['n_heads'],\n",
    "        n_layers=config['n_layers'],\n",
    "        d_ff=config['d_ff'],\n",
    "        max_seq_len=config['max_seq_len']\n",
    "    ).to(device)\n",
    "    \n",
    "    # 가중치 로드\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # 토크나이저 로드\n",
    "    with open(os.path.join(path, 'tokenizer.json'), 'r') as f:\n",
    "        tokenizer_data = json.load(f)\n",
    "    \n",
    "    tokenizer = SimpleTokenizer()\n",
    "    tokenizer.char_to_idx = tokenizer_data['char_to_idx']\n",
    "    tokenizer.idx_to_char = {int(k): v for k, v in tokenizer_data['idx_to_char'].items()}\n",
    "    tokenizer.vocab_size = tokenizer_data['vocab_size']\n",
    "    \n",
    "    print(f\"모델이 '{path}' 디렉토리에서 로드되었습니다.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# 모델 저장\n",
    "save_model(model, char_tokenizer)\n",
    "\n",
    "# 모델 불러오기 테스트\n",
    "loaded_model, loaded_tokenizer = load_model()\n",
    "print(\"\\n불러온 모델로 텍스트 생성:\")\n",
    "generate_sample(loaded_model, loaded_tokenizer, \"To die \", max_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 성능 분석 및 개선 방안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_performance(model, dataset, tokenizer):\n",
    "    \"\"\"모델 성능 분석\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Perplexity 계산\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Calculating perplexity\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            _, loss, _ = model(inputs, targets)\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0) * inputs.size(1)\n",
    "            total_tokens += inputs.size(0) * inputs.size(1)\n",
    "    \n",
    "    perplexity = math.exp(total_loss / total_tokens)\n",
    "    print(f\"\\nPerplexity: {perplexity:.2f}\")\n",
    "    \n",
    "    # 생성 품질 평가\n",
    "    print(\"\\n=== 생성 품질 평가 ===\")\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"To be or \",\n",
    "        \"Whether tis \",\n",
    "        \"The heart\",\n",
    "        \"To sleep \"\n",
    "    ]\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        print(f\"\\n프롬프트: '{prompt}'\")\n",
    "        tokens = tokenizer.encode(prompt)\n",
    "        x = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(x, 30, temperature=0.8)\n",
    "        \n",
    "        generated_text = tokenizer.decode(generated[0].tolist())\n",
    "        print(f\"생성: '{generated_text}'\")\n",
    "    \n",
    "    # 모델 통계\n",
    "    print(\"\\n=== 모델 통계 ===\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"총 파라미터 수: {total_params:,}\")\n",
    "    print(f\"학습 가능한 파라미터 수: {trainable_params:,}\")\n",
    "    print(f\"모델 크기: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # 레이어별 파라미터 분포\n",
    "    layer_params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_type = name.split('.')[0]\n",
    "        if layer_type not in layer_params:\n",
    "            layer_params[layer_type] = 0\n",
    "        layer_params[layer_type] += param.numel()\n",
    "    \n",
    "    print(\"\\n레이어별 파라미터 분포:\")\n",
    "    for layer, count in layer_params.items():\n",
    "        print(f\"  {layer}: {count:,} ({count/total_params*100:.1f}%)\")\n",
    "\n",
    "# 성능 분석 실행\n",
    "analyze_model_performance(model, dataset, char_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 개선 방안 및 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선 방안 시각화\n",
    "def visualize_improvements():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. 모델 크기 vs 성능\n",
    "    model_sizes = [1e6, 10e6, 100e6, 1e9, 10e9, 100e9]  # 파라미터 수\n",
    "    performance = [50, 35, 25, 15, 10, 7]  # Perplexity (낮을수록 좋음)\n",
    "    \n",
    "    axes[0, 0].loglog(model_sizes, performance, 'b-o', linewidth=2, markersize=8)\n",
    "    axes[0, 0].axvline(x=total_params, color='r', linestyle='--', label='Our Model')\n",
    "    axes[0, 0].set_xlabel('Number of Parameters')\n",
    "    axes[0, 0].set_ylabel('Perplexity')\n",
    "    axes[0, 0].set_title('Model Size vs Performance')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 학습 데이터 크기의 영향\n",
    "    data_sizes = [1e3, 1e4, 1e5, 1e6, 1e7, 1e8]  # 토큰 수\n",
    "    performance_data = [80, 50, 30, 20, 15, 12]\n",
    "    \n",
    "    axes[0, 1].loglog(data_sizes, performance_data, 'g-o', linewidth=2, markersize=8)\n",
    "    axes[0, 1].axvline(x=len(dataset.tokens), color='r', linestyle='--', label='Our Data')\n",
    "    axes[0, 1].set_xlabel('Training Tokens')\n",
    "    axes[0, 1].set_ylabel('Perplexity')\n",
    "    axes[0, 1].set_title('Data Size vs Performance')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. 다양한 아키텍처 비교\n",
    "    architectures = ['MLP', 'RNN', 'LSTM', 'Transformer', 'GPT-2', 'GPT-3']\n",
    "    params = [1e6, 10e6, 20e6, 100e6, 1.5e9, 175e9]\n",
    "    perplexities = [150, 80, 60, 30, 20, 10]\n",
    "    \n",
    "    axes[1, 0].scatter(params, perplexities, s=100, alpha=0.6)\n",
    "    for i, arch in enumerate(architectures):\n",
    "        axes[1, 0].annotate(arch, (params[i], perplexities[i]), \n",
    "                           xytext=(5, 5), textcoords='offset points')\n",
    "    axes[1, 0].set_xscale('log')\n",
    "    axes[1, 0].set_xlabel('Parameters')\n",
    "    axes[1, 0].set_ylabel('Perplexity')\n",
    "    axes[1, 0].set_title('Architecture Comparison')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. 개선 방안\n",
    "    improvements = [\n",
    "        'Larger Model',\n",
    "        'More Data',\n",
    "        'Better Tokenization',\n",
    "        'Longer Training',\n",
    "        'Advanced Techniques'\n",
    "    ]\n",
    "    expected_gains = [30, 40, 15, 10, 25]  # 예상 개선율 (%)\n",
    "    \n",
    "    axes[1, 1].barh(improvements, expected_gains, color='skyblue')\n",
    "    axes[1, 1].set_xlabel('Expected Improvement (%)')\n",
    "    axes[1, 1].set_title('Potential Improvements')\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_improvements()\n",
    "\n",
    "print(\"\\n=== 개선 방안 ===\")\n",
    "print(\"1. **모델 크기 증가**:\")\n",
    "print(\"   - 더 많은 레이어와 헤드 추가\")\n",
    "print(\"   - Hidden dimension 증가\")\n",
    "print(\"   - 현재: ~500K 파라미터 → 목표: 10M+ 파라미터\")\n",
    "print(\"\\n2. **데이터 증가**:\")\n",
    "print(\"   - 더 큰 텍스트 코퍼스 사용\")\n",
    "print(\"   - 다양한 장르와 스타일 포함\")\n",
    "print(\"   - 데이터 증강 기법 적용\")\n",
    "print(\"\\n3. **토크나이저 개선**:\")\n",
    "print(\"   - BPE (Byte-Pair Encoding) 사용\")\n",
    "print(\"   - SentencePiece 등 고급 토크나이저\")\n",
    "print(\"   - 더 효율적인 어휘 구성\")\n",
    "print(\"\\n4. **학습 기법 개선**:\")\n",
    "print(\"   - Mixed precision training\")\n",
    "print(\"   - Gradient accumulation\")\n",
    "print(\"   - Learning rate scheduling\")\n",
    "print(\"   - Warm-up steps\")\n",
    "print(\"\\n5. **고급 기법**:\")\n",
    "print(\"   - Flash Attention\")\n",
    "print(\"   - Rotary Position Embedding (RoPE)\")\n",
    "print(\"   - Layer-wise learning rate\")\n",
    "print(\"   - Knowledge distillation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 연습 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 1: Beam Search 구현\n",
    "def beam_search(model, tokenizer, prompt, beam_width=3, max_length=50):\n",
    "    \"\"\"\n",
    "    Beam search를 사용한 텍스트 생성\n",
    "    \n",
    "    힌트:\n",
    "    - 각 단계에서 beam_width개의 가장 확률 높은 시퀀스 유지\n",
    "    - 각 시퀀스의 로그 확률 합계 추적\n",
    "    - 최종적으로 가장 높은 점수의 시퀀스 반환\n",
    "    \"\"\"\n",
    "    # TODO: 구현하기\n",
    "    pass\n",
    "\n",
    "# 문제 2: 조건부 생성 구현\n",
    "class ConditionalGPT(MiniGPT):\n",
    "    \"\"\"\n",
    "    조건부 텍스트 생성을 위한 GPT\n",
    "    예: 감정, 스타일, 주제 등을 조건으로 제공\n",
    "    \n",
    "    힌트:\n",
    "    - 조건 임베딩 추가\n",
    "    - 조건과 텍스트 임베딩 결합\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, num_conditions, *args, **kwargs):\n",
    "        super().__init__(vocab_size, *args, **kwargs)\n",
    "        # TODO: 조건 임베딩 레이어 추가\n",
    "        pass\n",
    "\n",
    "# 문제 3: 효율적인 Attention 구현\n",
    "class EfficientAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    메모리 효율적인 Attention 구현\n",
    "    예: Sparse Attention, Local Attention 등\n",
    "    \n",
    "    힌트:\n",
    "    - 모든 위치가 아닌 일부만 attend\n",
    "    - 또는 청크 단위로 attention 계산\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, window_size=128):\n",
    "        super().__init__()\n",
    "        # TODO: 구현하기\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "이번 튜토리얼에서 배운 내용:\n",
    "1. **GPT 아키텍처**: Decoder-only Transformer\n",
    "2. **토큰화**: 텍스트를 모델이 이해할 수 있는 형태로 변환\n",
    "3. **학습 파이프라인**: 데이터 준비부터 모델 학습까지\n",
    "4. **텍스트 생성**: 다양한 샘플링 전략\n",
    "5. **성능 분석**: Perplexity와 생성 품질 평가\n",
    "\n",
    "### Mini GPT의 핵심 구성 요소:\n",
    "- **Self-Attention**: 이전 토큰들의 정보 활용\n",
    "- **Causal Masking**: 미래 정보 차단\n",
    "- **Position Embedding**: 순서 정보 인코딩\n",
    "- **Autoregressive Generation**: 한 토큰씩 순차적 생성\n",
    "\n",
    "### 실제 GPT와의 차이:\n",
    "- **모델 크기**: 실제 GPT는 수십억~수천억 파라미터\n",
    "- **데이터**: 인터넷 규모의 텍스트로 학습\n",
    "- **토크나이저**: BPE 등 더 효율적인 방법 사용\n",
    "- **최적화**: 분산 학습, mixed precision 등\n",
    "\n",
    "다음 단계에서는 이러한 대규모 언어 모델을 실제로 사용하는 방법을 배워보겠습니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}